{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ML2 Coursework - Complete Pipeline\n",
        "\n",
        "This notebook combines all stages of the coursework into one coherent end-to-end workflow:\n",
        "1. Data loading and initial preparation\n",
        "2. Dataset creation (A0 and A1)\n",
        "3. Feature engineering\n",
        "4. Data preprocessing\n",
        "5. Model creation and training\n",
        "6. Model evaluation\n",
        "7. Results and conclusions\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction and Setup\n",
        "\n",
        "### Libraries Import\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Core libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import datetime\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, TimeSeriesSplit\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, confusion_matrix, \n",
        "    precision_recall_curve, average_precision_score, roc_auc_score,\n",
        "    mean_squared_error\n",
        ")\n",
        "\n",
        "# XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Statistical functions\n",
        "import scipy.stats as st\n",
        "\n",
        "# Set random state for reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "# Suppress warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "np.seterr(all=\"ignore\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Initial Preparation\n",
        "\n",
        "We start by loading raw data files from the directory structure and combining them into country-level datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined CSVs already exist. Loading from disk...\n"
          ]
        }
      ],
      "source": [
        "# Check if combined CSVs already exist, otherwise create them\n",
        "# Try both possible directory names\n",
        "root_dir = None\n",
        "for possible_dir in [\"raw_data\", \"data\"]:\n",
        "    if os.path.exists(possible_dir):\n",
        "        root_dir = possible_dir\n",
        "        break\n",
        "\n",
        "if root_dir is None:\n",
        "    raise FileNotFoundError(\"Neither 'raw_data' nor 'data' directory found. Please check your data directory.\")\n",
        "\n",
        "output_dir = \"combined_csvs\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Check if combined CSVs exist\n",
        "combined_files_exist = all(\n",
        "    os.path.exists(os.path.join(output_dir, f\"{country}.csv\"))\n",
        "    for country in [\"belgium\", \"england\", \"france\", \"germany\", \"greece\", \n",
        "                    \"italy\", \"netherlands\", \"portugal\", \"scotland\", \"spain\", \"turkey\"]\n",
        ")\n",
        "\n",
        "if not combined_files_exist:\n",
        "    print(\"Combined CSVs not found. Creating them from raw data...\")\n",
        "    \n",
        "    folder_dfs = {}  # Nested dict: country -> subfolder -> CSV -> DataFrame\n",
        "    \n",
        "    # Walk through all directories\n",
        "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
        "        if dirpath == root_dir:\n",
        "            continue  # skip root\n",
        "        \n",
        "        # Split the path components relative to root_dir\n",
        "        rel_path = os.path.relpath(dirpath, root_dir).split(os.sep)\n",
        "        \n",
        "        # Expecting: [\"england\", \"0\"] or [\"england\", \"1\"], etc.\n",
        "        if len(rel_path) != 2:\n",
        "            continue  # skip if not exactly two levels below root\n",
        "        \n",
        "        country, subfolder = rel_path\n",
        "        \n",
        "        # Initialize nested dicts\n",
        "        folder_dfs.setdefault(country, {})\n",
        "        folder_dfs[country].setdefault(subfolder, {})\n",
        "        \n",
        "        # Loop through CSV files\n",
        "        for filename in filenames:\n",
        "            if filename.endswith(\".csv\"):\n",
        "                file_path = os.path.join(dirpath, filename)\n",
        "                try:\n",
        "                    df = pd.read_csv(file_path, low_memory=False)\n",
        "                    csv_name = os.path.splitext(filename)[0]  # e.g. \"1920\"\n",
        "                    folder_dfs[country][subfolder][csv_name] = df\n",
        "                except Exception as e:\n",
        "                    print(f\"Error reading {file_path}: {e}\")\n",
        "    \n",
        "    def concat_country_data(country_name):\n",
        "        \"\"\"Concatenate all CSVs for a given country (across all subfolders).\"\"\"\n",
        "        if country_name not in folder_dfs:\n",
        "            return pd.DataFrame()\n",
        "        # Flatten all subfolder DataFrames into one list\n",
        "        dfs = []\n",
        "        for subfolder in folder_dfs[country_name].values():\n",
        "            dfs.extend(subfolder.values())\n",
        "        # Combine them into one big DataFrame\n",
        "        if dfs:\n",
        "            return pd.concat(dfs, ignore_index=True)\n",
        "        return pd.DataFrame()\n",
        "    \n",
        "    # Build DataFrames per country\n",
        "    belgium_data = concat_country_data(\"belgium\")\n",
        "    england_data = concat_country_data(\"england\")\n",
        "    france_data = concat_country_data(\"france\")\n",
        "    germany_data = concat_country_data(\"germany\")\n",
        "    greece_data = concat_country_data(\"greece\")\n",
        "    italy_data = concat_country_data(\"italy\")\n",
        "    netherlands_data = concat_country_data(\"netherlands\")\n",
        "    portugal_data = concat_country_data(\"portugal\")\n",
        "    scotland_data = concat_country_data(\"scotland\")\n",
        "    spain_data = concat_country_data(\"spain\")\n",
        "    turkey_data = concat_country_data(\"turkey\")\n",
        "    \n",
        "    country_dfs = {\n",
        "        \"belgium\": belgium_data,\n",
        "        \"england\": england_data,\n",
        "        \"france\": france_data,\n",
        "        \"germany\": germany_data,\n",
        "        \"greece\": greece_data,\n",
        "        \"italy\": italy_data,\n",
        "        \"netherlands\": netherlands_data,\n",
        "        \"portugal\": portugal_data,\n",
        "        \"scotland\": scotland_data,\n",
        "        \"spain\": spain_data,\n",
        "        \"turkey\": turkey_data\n",
        "    }\n",
        "    \n",
        "    # Save combined CSVs\n",
        "    for country, df in country_dfs.items():\n",
        "        if not df.empty:\n",
        "            file_path = os.path.join(output_dir, f\"{country}.csv\")\n",
        "            df.to_csv(file_path, index=False)\n",
        "            print(f\"Saved {file_path} (shape={df.shape})\")\n",
        "    \n",
        "    print(\"\\nCombined CSVs created successfully!\")\n",
        "else:\n",
        "    print(\"Combined CSVs already exist. Loading from disk...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total dataset shape: (42593, 137)\n",
            "Columns: ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'HTHG', 'HTAG']...\n"
          ]
        }
      ],
      "source": [
        "# Load combined country data\n",
        "b_data = pd.read_csv(os.path.join(output_dir, 'belgium.csv'), low_memory=False)\n",
        "eng_data = pd.read_csv(os.path.join(output_dir, 'england.csv'), low_memory=False)\n",
        "fr_data = pd.read_csv(os.path.join(output_dir, 'france.csv'), low_memory=False)\n",
        "d_data = pd.read_csv(os.path.join(output_dir, 'germany.csv'), low_memory=False)\n",
        "gr_data = pd.read_csv(os.path.join(output_dir, 'greece.csv'), low_memory=False)\n",
        "it_data = pd.read_csv(os.path.join(output_dir, 'italy.csv'), low_memory=False)\n",
        "ne_data = pd.read_csv(os.path.join(output_dir, 'netherlands.csv'), low_memory=False)\n",
        "por_data = pd.read_csv(os.path.join(output_dir, 'portugal.csv'), low_memory=False)\n",
        "sc_data = pd.read_csv(os.path.join(output_dir, 'scotland.csv'), low_memory=False)\n",
        "sp_data = pd.read_csv(os.path.join(output_dir, 'spain.csv'), low_memory=False)\n",
        "tur_data = pd.read_csv(os.path.join(output_dir, 'turkey.csv'), low_memory=False)\n",
        "\n",
        "# Combine all countries into one dataset\n",
        "data_og = pd.DataFrame()\n",
        "for df in [b_data, eng_data, fr_data, d_data, gr_data, it_data, ne_data, por_data, sc_data, sp_data, tur_data]:\n",
        "    data_og = pd.concat([data_og, df], ignore_index=True)\n",
        "\n",
        "print(f\"Total dataset shape: {data_og.shape}\")\n",
        "print(f\"Columns: {data_og.columns.tolist()[:10]}...\")  # Show first 10 columns\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Dataset Creation (A0 and A1)\n",
        "\n",
        "We create two datasets:\n",
        "- **A0**: Limited features (basic match information)\n",
        "- **A1**: Extended features with betting odds and market aggregates\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.1 A0 Dataset Creation\n",
        "\n",
        "A0 dataset uses only basic features: Div, Date, Time, HomeTeam, AwayTeam, FTHG, FTAG, FTR\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A0 dataset shape: (42593, 11)\n",
            "A0 columns: ['Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'Country', 'Division', 'Total_goals', 'Target']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>HomeTeam</th>\n",
              "      <th>AwayTeam</th>\n",
              "      <th>FTHG</th>\n",
              "      <th>FTAG</th>\n",
              "      <th>FTR</th>\n",
              "      <th>Country</th>\n",
              "      <th>Division</th>\n",
              "      <th>Total_goals</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2019-07-26</td>\n",
              "      <td>19</td>\n",
              "      <td>Genk</td>\n",
              "      <td>Kortrijk</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>H</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2019-07-27</td>\n",
              "      <td>17</td>\n",
              "      <td>Cercle Brugge</td>\n",
              "      <td>Standard</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2019-07-27</td>\n",
              "      <td>19</td>\n",
              "      <td>St Truiden</td>\n",
              "      <td>Mouscron</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2019-07-27</td>\n",
              "      <td>19</td>\n",
              "      <td>Waregem</td>\n",
              "      <td>Mechelen</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2019-07-27</td>\n",
              "      <td>19</td>\n",
              "      <td>Waasland-Beveren</td>\n",
              "      <td>Club Brugge</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>A</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Date  Time          HomeTeam     AwayTeam  FTHG  FTAG FTR Country  \\\n",
              "0 2019-07-26    19              Genk     Kortrijk     2     1   H       B   \n",
              "1 2019-07-27    17     Cercle Brugge     Standard     0     2   A       B   \n",
              "2 2019-07-27    19        St Truiden     Mouscron     0     1   A       B   \n",
              "3 2019-07-27    19           Waregem     Mechelen     0     2   A       B   \n",
              "4 2019-07-27    19  Waasland-Beveren  Club Brugge     1     3   A       B   \n",
              "\n",
              "  Division  Total_goals  Target  \n",
              "0        1            3       1  \n",
              "1        1            2       0  \n",
              "2        1            1       0  \n",
              "3        1            2       0  \n",
              "4        1            4       1  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "col_list_a0 = ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR']\n",
        "\n",
        "def a0_data(df, feature_list):\n",
        "    \"\"\"Create A0 dataset with basic features only.\"\"\"\n",
        "    df = df[feature_list].copy()\n",
        "    # Extract Country and Division from Div column\n",
        "    df[['Country', 'Division']] = df['Div'].str.extract(r'([A-Za-z]+)(\\d+)')\n",
        "    df.drop(columns=['Div'], inplace=True)\n",
        "    # Create target variable\n",
        "    df['Total_goals'] = df['FTHG'] + df['FTAG']\n",
        "    df['Target'] = (df['Total_goals'] > 2.5).astype(int)  # 1 if more than 2.5 goals, else 0\n",
        "    # Convert Date and Time to proper formats\n",
        "    df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
        "    df['Time'] = pd.to_datetime(df['Time'], format='%H:%M').dt.hour\n",
        "    return df\n",
        "\n",
        "# Create A0 dataset for all countries\n",
        "data_og_a0 = a0_data(data_og, col_list_a0)\n",
        "print(f\"A0 dataset shape: {data_og_a0.shape}\")\n",
        "print(f\"A0 columns: {data_og_a0.columns.tolist()}\")\n",
        "data_og_a0.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 A1 Dataset Creation\n",
        "\n",
        "A1 dataset includes betting odds and market aggregates: AvgAHH, AvgAHA, Avg>2.5, Avg<2.5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1 dataset shape: (42593, 15)\n",
            "A1 columns: ['Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', 'AvgAHH', 'AvgAHA', 'Avg>2.5', 'Avg<2.5', 'Country', 'Division', 'Total_goals', 'Target']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Date</th>\n",
              "      <th>Time</th>\n",
              "      <th>HomeTeam</th>\n",
              "      <th>AwayTeam</th>\n",
              "      <th>FTHG</th>\n",
              "      <th>FTAG</th>\n",
              "      <th>FTR</th>\n",
              "      <th>AvgAHH</th>\n",
              "      <th>AvgAHA</th>\n",
              "      <th>Avg&gt;2.5</th>\n",
              "      <th>Avg&lt;2.5</th>\n",
              "      <th>Country</th>\n",
              "      <th>Division</th>\n",
              "      <th>Total_goals</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>26/07/2019</td>\n",
              "      <td>19:30</td>\n",
              "      <td>Genk</td>\n",
              "      <td>Kortrijk</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>H</td>\n",
              "      <td>2.05</td>\n",
              "      <td>1.79</td>\n",
              "      <td>1.51</td>\n",
              "      <td>2.48</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>27/07/2019</td>\n",
              "      <td>17:00</td>\n",
              "      <td>Cercle Brugge</td>\n",
              "      <td>Standard</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>A</td>\n",
              "      <td>1.90</td>\n",
              "      <td>1.93</td>\n",
              "      <td>1.74</td>\n",
              "      <td>2.06</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>27/07/2019</td>\n",
              "      <td>19:00</td>\n",
              "      <td>St Truiden</td>\n",
              "      <td>Mouscron</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>A</td>\n",
              "      <td>1.95</td>\n",
              "      <td>1.88</td>\n",
              "      <td>1.82</td>\n",
              "      <td>1.98</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>27/07/2019</td>\n",
              "      <td>19:00</td>\n",
              "      <td>Waregem</td>\n",
              "      <td>Mechelen</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>A</td>\n",
              "      <td>1.92</td>\n",
              "      <td>1.91</td>\n",
              "      <td>1.65</td>\n",
              "      <td>2.19</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>27/07/2019</td>\n",
              "      <td>19:30</td>\n",
              "      <td>Waasland-Beveren</td>\n",
              "      <td>Club Brugge</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>A</td>\n",
              "      <td>1.96</td>\n",
              "      <td>1.87</td>\n",
              "      <td>1.55</td>\n",
              "      <td>2.38</td>\n",
              "      <td>B</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Date   Time          HomeTeam     AwayTeam  FTHG  FTAG FTR  AvgAHH  \\\n",
              "0  26/07/2019  19:30              Genk     Kortrijk     2     1   H    2.05   \n",
              "1  27/07/2019  17:00     Cercle Brugge     Standard     0     2   A    1.90   \n",
              "2  27/07/2019  19:00        St Truiden     Mouscron     0     1   A    1.95   \n",
              "3  27/07/2019  19:00           Waregem     Mechelen     0     2   A    1.92   \n",
              "4  27/07/2019  19:30  Waasland-Beveren  Club Brugge     1     3   A    1.96   \n",
              "\n",
              "   AvgAHA  Avg>2.5  Avg<2.5 Country Division  Total_goals  Target  \n",
              "0    1.79     1.51     2.48       B        1            3       1  \n",
              "1    1.93     1.74     2.06       B        1            2       0  \n",
              "2    1.88     1.82     1.98       B        1            1       0  \n",
              "3    1.91     1.65     2.19       B        1            2       0  \n",
              "4    1.87     1.55     2.38       B        1            4       1  "
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "col_list_a1 = ['Div', 'Date', 'Time', 'HomeTeam', 'AwayTeam', 'FTHG', 'FTAG', 'FTR', \n",
        "               'AvgAHH', 'AvgAHA', 'Avg>2.5', 'Avg<2.5']\n",
        "\n",
        "def a1_data(df, feature_list):\n",
        "    \"\"\"Create A1 dataset with betting odds features.\"\"\"\n",
        "    # Check which columns are available\n",
        "    available_cols = [col for col in feature_list if col in df.columns]\n",
        "    df = df[available_cols].copy()\n",
        "    \n",
        "    # Extract Country and Division from Div column\n",
        "    df[['Country', 'Division']] = df['Div'].str.extract(r'([A-Za-z]+)(\\d+)')\n",
        "    df.drop(columns=['Div'], inplace=True)\n",
        "    \n",
        "    # Create target variable\n",
        "    df['Total_goals'] = df['FTHG'] + df['FTAG']\n",
        "    df['Target'] = (df['Total_goals'] > 2.5).astype(int)  # 1 if more than 2.5 goals, else 0\n",
        "    \n",
        "    return df\n",
        "\n",
        "# Create A1 dataset for all countries\n",
        "data_big = pd.DataFrame()\n",
        "for df in [b_data, eng_data, fr_data, d_data, gr_data, it_data, ne_data, por_data, sc_data, sp_data, tur_data]:\n",
        "    data_big = pd.concat([data_big, df], ignore_index=True)\n",
        "\n",
        "big_data_a1 = a1_data(data_big, col_list_a1)\n",
        "print(f\"A1 dataset shape: {big_data_a1.shape}\")\n",
        "print(f\"A1 columns: {big_data_a1.columns.tolist()}\")\n",
        "big_data_a1.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Feature Engineering\n",
        "\n",
        "We now engineer features for both A0 and A1 datasets, including:\n",
        "- Calendar features (Year, Month, Dayofweek, Is_weekend, Season_of_year)\n",
        "- Team encoding (LabelEncoder)\n",
        "- Rolling averages (last 5 matches)\n",
        "- Market features (probabilities, normalization)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_season(date):\n",
        "    \"\"\"Get season of year: 0=winter, 1=spring, 2=summer, 3=autumn\"\"\"\n",
        "    month = date.month\n",
        "    if month in [12, 1, 2]:\n",
        "        return 0\n",
        "    elif month in [3, 4, 5]:\n",
        "        return 1\n",
        "    elif month in [6, 7, 8]:\n",
        "        return 2\n",
        "    else:\n",
        "        return 3\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transf_encode_a0(df):\n",
        "    \"\"\"Transform and encode A0 dataset features.\"\"\"\n",
        "    # Sort by date to ensure chronological order for rolling averages\n",
        "    df = df.sort_values('Date').copy()\n",
        "    \n",
        "    # Team encoding - use same encoder for both HomeTeam and AwayTeam\n",
        "    team_encoder = LabelEncoder()\n",
        "    all_teams = pd.concat([df['HomeTeam'], df['AwayTeam']]).unique()\n",
        "    team_encoder.fit(all_teams)\n",
        "    \n",
        "    # Home team features\n",
        "    df['HomeTeam_enc'] = team_encoder.transform(df['HomeTeam'])\n",
        "    df = df.sort_values([\"HomeTeam_enc\", \"Date\"])\n",
        "    df['avg_goals_in_last5_home'] = (\n",
        "        df.groupby(\"HomeTeam_enc\")[\"FTHG\"]\n",
        "        .transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
        "    )\n",
        "    df['avg_goals_conceded_last5_home'] = (\n",
        "        df.groupby(\"HomeTeam_enc\")[\"FTAG\"]\n",
        "        .transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
        "    )\n",
        "    \n",
        "    # Away team features\n",
        "    df['AwayTeam_enc'] = team_encoder.transform(df['AwayTeam'])\n",
        "    df = df.sort_values([\"AwayTeam_enc\", \"Date\"])\n",
        "    df['avg_goals_in_last5_away'] = (\n",
        "        df.groupby(\"AwayTeam_enc\")[\"FTAG\"]\n",
        "        .transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
        "    )\n",
        "    df['avg_goals_conceded_last5_away'] = (\n",
        "        df.groupby(\"AwayTeam_enc\")[\"FTHG\"]\n",
        "        .transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
        "    )\n",
        "    \n",
        "    # Sort back to chronological order\n",
        "    df = df.sort_values('Date')\n",
        "    \n",
        "    # Calendar-based features\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Dayofweek'] = df['Date'].dt.dayofweek  # Monday=0, Sunday=6\n",
        "    df['Is_weekend'] = df['Dayofweek'].isin([5, 6]).astype(int)\n",
        "    df['Season_of_year'] = df['Date'].apply(get_season)\n",
        "    \n",
        "    # Encode Country and Division as dummies\n",
        "    if 'Country' in df.columns:\n",
        "        df = pd.get_dummies(df, columns=['Country', 'Division'], drop_first=True)\n",
        "    else:\n",
        "        df = pd.get_dummies(df, columns=['Division'], drop_first=True)\n",
        "        if 'Country' in df.columns:\n",
        "            df = df.drop(columns=['Country'])\n",
        "    \n",
        "    # Drop columns used in calculations\n",
        "    df = df.drop(columns=['Total_goals', 'FTHG', 'FTAG', 'FTR', 'HomeTeam', 'AwayTeam', 'Date'])\n",
        "    \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def transf_encode_a1(df):\n",
        "    \"\"\"Transform and encode A1 dataset features with betting odds.\"\"\"\n",
        "    # Sort by date\n",
        "    df = df.sort_values('Date').copy()\n",
        "    \n",
        "    # Convert Date and Time if not already converted\n",
        "    if not pd.api.types.is_datetime64_any_dtype(df['Date']):\n",
        "        df['Date'] = pd.to_datetime(df['Date'], format='%d/%m/%Y')\n",
        "    if not isinstance(df['Time'].iloc[0], (int, np.integer)):\n",
        "        df['Time'] = pd.to_datetime(df['Time'], format='%H:%M').dt.hour\n",
        "    \n",
        "    # Team encoding\n",
        "    team_encoder = LabelEncoder()\n",
        "    all_teams = pd.concat([df['HomeTeam'], df['AwayTeam']]).unique()\n",
        "    team_encoder.fit(all_teams)\n",
        "    \n",
        "    # Home team features\n",
        "    df['HomeTeam_enc'] = team_encoder.transform(df['HomeTeam'])\n",
        "    df = df.sort_values([\"HomeTeam_enc\", \"Date\"])\n",
        "    df['avg_goals_in_last5_home'] = (\n",
        "        df.groupby(\"HomeTeam_enc\")[\"FTHG\"]\n",
        "        .transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
        "    )\n",
        "    df['avg_goals_conceded_last5_home'] = (\n",
        "        df.groupby(\"HomeTeam_enc\")[\"FTAG\"]\n",
        "        .transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
        "    )\n",
        "    \n",
        "    # Away team features\n",
        "    df['AwayTeam_enc'] = team_encoder.transform(df['AwayTeam'])\n",
        "    df = df.sort_values([\"AwayTeam_enc\", \"Date\"])\n",
        "    df['avg_goals_in_last5_away'] = (\n",
        "        df.groupby(\"AwayTeam_enc\")[\"FTAG\"]\n",
        "        .transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
        "    )\n",
        "    df['avg_goals_conceded_last5_away'] = (\n",
        "        df.groupby(\"AwayTeam_enc\")[\"FTHG\"]\n",
        "        .transform(lambda x: x.shift().rolling(5, min_periods=1).mean())\n",
        "    )\n",
        "    \n",
        "    # Sort back to chronological order\n",
        "    df = df.sort_values('Date')\n",
        "    \n",
        "    # Over/Under market features\n",
        "    if 'Avg>2.5' in df.columns and 'Avg<2.5' in df.columns:\n",
        "        # Convert odds to probabilities\n",
        "        df[\"p_over_raw\"] = 1 / df[\"Avg>2.5\"]\n",
        "        df[\"p_under_raw\"] = 1 / df[\"Avg<2.5\"]\n",
        "        # Normalize to remove overround\n",
        "        df[\"p_over_norm\"] = df[\"p_over_raw\"] / (df[\"p_over_raw\"] + df[\"p_under_raw\"])\n",
        "        df[\"p_under_norm\"] = 1 - df[\"p_over_norm\"]\n",
        "        \n",
        "        # Key modeling features\n",
        "        df[\"P_over\"] = df[\"p_over_norm\"]\n",
        "        df[\"market_decisiveness\"] = (df[\"P_over\"] - 0.5).abs()\n",
        "        # Expected total goals based on market probability\n",
        "        df[\"expected_total_goals\"] = 2.5 + np.log(df['P_over'] + 1e-9) / (1 - (df[\"P_over\"] - 0.5) + 1e-9)\n",
        "    \n",
        "    # Asian Handicap features\n",
        "    if 'AvgAHH' in df.columns and 'AvgAHA' in df.columns:\n",
        "        # Filter out invalid odds (must be > 1.0)\n",
        "        df = df[df['AvgAHH'] > 1.0].copy()\n",
        "        df = df[df['AvgAHA'] > 1.0].copy()\n",
        "        \n",
        "        # Convert odds to probabilities\n",
        "        df['Ah_P_home'] = 1 / df['AvgAHH']\n",
        "        df['Ah_P_away'] = 1 / df['AvgAHA']\n",
        "        \n",
        "        # Normalized probabilities\n",
        "        df['Norm_Ah_P_home'] = df['Ah_P_home'] / (df['Ah_P_home'] + df['Ah_P_away'])\n",
        "        df['Norm_Ah_P_away'] = 1 - df['Norm_Ah_P_home']\n",
        "        \n",
        "        # Market imbalance and confidence\n",
        "        df['ah_imbalance'] = (df['Ah_P_home'] - df['Ah_P_away']).abs()\n",
        "        df['ah_market_confidence'] = df[['Ah_P_home', 'Ah_P_away']].max(axis=1)\n",
        "    \n",
        "    # Calendar-based features\n",
        "    df['Year'] = df['Date'].dt.year\n",
        "    df['Month'] = df['Date'].dt.month\n",
        "    df['Dayofweek'] = df['Date'].dt.dayofweek\n",
        "    df['Is_weekend'] = df['Dayofweek'].isin([5, 6]).astype(int)\n",
        "    df['Season_of_year'] = df['Date'].apply(get_season)\n",
        "    \n",
        "    # Encode Country and Division as dummies\n",
        "    if 'Country' in df.columns:\n",
        "        df = pd.get_dummies(df, columns=['Country', 'Division'], drop_first=True)\n",
        "    else:\n",
        "        df = pd.get_dummies(df, columns=['Division'], drop_first=True)\n",
        "        if 'Country' in df.columns:\n",
        "            df = df.drop(columns=['Country'])\n",
        "    \n",
        "    # Drop unneeded columns\n",
        "    cols_to_drop = [\n",
        "        'Total_goals', 'FTHG', 'FTAG', 'FTR', 'HomeTeam', 'AwayTeam', 'Date',\n",
        "        'p_over_raw', 'p_under_raw', 'p_over_norm', 'p_under_norm',\n",
        "        'over_price', 'P_over',\n",
        "        'Ah_P_home', 'Ah_P_away',\n",
        "        'Avg>2.5', 'Avg<2.5', 'AvgAHH', 'AvgAHA'\n",
        "    ]\n",
        "    # Only drop columns that exist\n",
        "    cols_to_drop = [col for col in cols_to_drop if col in df.columns]\n",
        "    df = df.drop(columns=cols_to_drop)\n",
        "    \n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A0 encoded shape: (42593, 26)\n",
            "A0 encoded columns: ['Time', 'Target', 'HomeTeam_enc', 'avg_goals_in_last5_home', 'avg_goals_conceded_last5_home', 'AwayTeam_enc', 'avg_goals_in_last5_away', 'avg_goals_conceded_last5_away', 'Year', 'Month', 'Dayofweek', 'Is_weekend', 'Season_of_year', 'Country_D', 'Country_E', 'Country_F', 'Country_G', 'Country_I', 'Country_N', 'Country_P', 'Country_SC', 'Country_SP', 'Country_T', 'Division_1', 'Division_2', 'Division_3']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>Target</th>\n",
              "      <th>HomeTeam_enc</th>\n",
              "      <th>avg_goals_in_last5_home</th>\n",
              "      <th>avg_goals_conceded_last5_home</th>\n",
              "      <th>AwayTeam_enc</th>\n",
              "      <th>avg_goals_in_last5_away</th>\n",
              "      <th>avg_goals_conceded_last5_away</th>\n",
              "      <th>Year</th>\n",
              "      <th>Month</th>\n",
              "      <th>...</th>\n",
              "      <th>Country_G</th>\n",
              "      <th>Country_I</th>\n",
              "      <th>Country_N</th>\n",
              "      <th>Country_P</th>\n",
              "      <th>Country_SC</th>\n",
              "      <th>Country_SP</th>\n",
              "      <th>Country_T</th>\n",
              "      <th>Division_1</th>\n",
              "      <th>Division_2</th>\n",
              "      <th>Division_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19697</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>420</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>204</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15758</th>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>101</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>444</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15757</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>245</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15760</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>201</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>196</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15763</th>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>372</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>44</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2019</td>\n",
              "      <td>7</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 26 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Time  Target  HomeTeam_enc  avg_goals_in_last5_home  \\\n",
              "19697    19       1           420                      NaN   \n",
              "15758    19       0           101                      NaN   \n",
              "15757    19       1             8                      NaN   \n",
              "15760    19       1           201                      NaN   \n",
              "15763    19       0           372                      NaN   \n",
              "\n",
              "       avg_goals_conceded_last5_home  AwayTeam_enc  avg_goals_in_last5_away  \\\n",
              "19697                            NaN           204                      NaN   \n",
              "15758                            NaN           444                      NaN   \n",
              "15757                            NaN           245                      NaN   \n",
              "15760                            NaN           196                      NaN   \n",
              "15763                            NaN            44                      NaN   \n",
              "\n",
              "       avg_goals_conceded_last5_away  Year  Month  ...  Country_G  Country_I  \\\n",
              "19697                            NaN  2019      7  ...      False      False   \n",
              "15758                            NaN  2019      7  ...      False      False   \n",
              "15757                            NaN  2019      7  ...      False      False   \n",
              "15760                            NaN  2019      7  ...      False      False   \n",
              "15763                            NaN  2019      7  ...      False      False   \n",
              "\n",
              "       Country_N  Country_P  Country_SC  Country_SP  Country_T  Division_1  \\\n",
              "19697      False      False       False       False      False       False   \n",
              "15758      False      False       False       False      False       False   \n",
              "15757      False      False       False       False      False       False   \n",
              "15760      False      False       False       False      False       False   \n",
              "15763      False      False       False       False      False       False   \n",
              "\n",
              "       Division_2  Division_3  \n",
              "19697        True       False  \n",
              "15758        True       False  \n",
              "15757        True       False  \n",
              "15760        True       False  \n",
              "15763        True       False  \n",
              "\n",
              "[5 rows x 26 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply feature engineering to A0 dataset\n",
        "data_og_a0_encoded = transf_encode_a0(data_og_a0.copy())\n",
        "print(f\"A0 encoded shape: {data_og_a0_encoded.shape}\")\n",
        "print(f\"A0 encoded columns: {data_og_a0_encoded.columns.tolist()}\")\n",
        "data_og_a0_encoded.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A1 encoded shape: (42545, 32)\n",
            "A1 encoded columns: ['Time', 'Target', 'HomeTeam_enc', 'avg_goals_in_last5_home', 'avg_goals_conceded_last5_home', 'AwayTeam_enc', 'avg_goals_in_last5_away', 'avg_goals_conceded_last5_away', 'market_decisiveness', 'expected_total_goals', 'Norm_Ah_P_home', 'Norm_Ah_P_away', 'ah_imbalance', 'ah_market_confidence', 'Year', 'Month', 'Dayofweek', 'Is_weekend', 'Season_of_year', 'Country_D', 'Country_E', 'Country_F', 'Country_G', 'Country_I', 'Country_N', 'Country_P', 'Country_SC', 'Country_SP', 'Country_T', 'Division_1', 'Division_2', 'Division_3']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Time</th>\n",
              "      <th>Target</th>\n",
              "      <th>HomeTeam_enc</th>\n",
              "      <th>avg_goals_in_last5_home</th>\n",
              "      <th>avg_goals_conceded_last5_home</th>\n",
              "      <th>AwayTeam_enc</th>\n",
              "      <th>avg_goals_in_last5_away</th>\n",
              "      <th>avg_goals_conceded_last5_away</th>\n",
              "      <th>market_decisiveness</th>\n",
              "      <th>expected_total_goals</th>\n",
              "      <th>...</th>\n",
              "      <th>Country_G</th>\n",
              "      <th>Country_I</th>\n",
              "      <th>Country_N</th>\n",
              "      <th>Country_P</th>\n",
              "      <th>Country_SC</th>\n",
              "      <th>Country_SP</th>\n",
              "      <th>Country_T</th>\n",
              "      <th>Division_1</th>\n",
              "      <th>Division_2</th>\n",
              "      <th>Division_3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>19697</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>420</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>204</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.066234</td>\n",
              "      <td>1.890909</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15758</th>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>101</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>444</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.056136</td>\n",
              "      <td>1.730935</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15757</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>8</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>245</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.144769</td>\n",
              "      <td>1.595899</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15760</th>\n",
              "      <td>19</td>\n",
              "      <td>1</td>\n",
              "      <td>201</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>196</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.070312</td>\n",
              "      <td>1.710794</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15763</th>\n",
              "      <td>19</td>\n",
              "      <td>0</td>\n",
              "      <td>372</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>44</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.114610</td>\n",
              "      <td>1.644545</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 32 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Time  Target  HomeTeam_enc  avg_goals_in_last5_home  \\\n",
              "19697    19       1           420                      NaN   \n",
              "15758    19       0           101                      NaN   \n",
              "15757    19       1             8                      NaN   \n",
              "15760    19       1           201                      NaN   \n",
              "15763    19       0           372                      NaN   \n",
              "\n",
              "       avg_goals_conceded_last5_home  AwayTeam_enc  avg_goals_in_last5_away  \\\n",
              "19697                            NaN           204                      NaN   \n",
              "15758                            NaN           444                      NaN   \n",
              "15757                            NaN           245                      NaN   \n",
              "15760                            NaN           196                      NaN   \n",
              "15763                            NaN            44                      NaN   \n",
              "\n",
              "       avg_goals_conceded_last5_away  market_decisiveness  \\\n",
              "19697                            NaN             0.066234   \n",
              "15758                            NaN             0.056136   \n",
              "15757                            NaN             0.144769   \n",
              "15760                            NaN             0.070312   \n",
              "15763                            NaN             0.114610   \n",
              "\n",
              "       expected_total_goals  ...  Country_G  Country_I  Country_N  Country_P  \\\n",
              "19697              1.890909  ...      False      False      False      False   \n",
              "15758              1.730935  ...      False      False      False      False   \n",
              "15757              1.595899  ...      False      False      False      False   \n",
              "15760              1.710794  ...      False      False      False      False   \n",
              "15763              1.644545  ...      False      False      False      False   \n",
              "\n",
              "       Country_SC  Country_SP  Country_T  Division_1  Division_2  Division_3  \n",
              "19697       False       False      False       False        True       False  \n",
              "15758       False       False      False       False        True       False  \n",
              "15757       False       False      False       False        True       False  \n",
              "15760       False       False      False       False        True       False  \n",
              "15763       False       False      False       False        True       False  \n",
              "\n",
              "[5 rows x 32 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Apply feature engineering to A1 dataset\n",
        "data_big_a1_encoded = transf_encode_a1(big_data_a1.copy())\n",
        "print(f\"A1 encoded shape: {data_big_a1_encoded.shape}\")\n",
        "print(f\"A1 encoded columns: {data_big_a1_encoded.columns.tolist()}\")\n",
        "data_big_a1_encoded.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Data Preprocessing\n",
        "\n",
        "Before model training, we need to:\n",
        "- Fix data types (boolean dummies to int)\n",
        "- Remove infinity values\n",
        "- Handle missing values\n",
        "- Sort chronologically\n",
        "- Split into train/test sets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data preprocessing complete.\n",
            "A0 shape: (42593, 26)\n",
            "A1 shape: (42545, 32)\n",
            "\n",
            "Non-numeric columns in A0: []\n",
            "Non-numeric columns in A1: []\n"
          ]
        }
      ],
      "source": [
        "# Fix boolean dummies to int\n",
        "for df in [data_og_a0_encoded, data_big_a1_encoded]:\n",
        "    bool_cols = df.select_dtypes(include='bool').columns\n",
        "    if len(bool_cols) > 0:\n",
        "        df[bool_cols] = df[bool_cols].astype(int)\n",
        "\n",
        "# Replace infinity values with NaN\n",
        "for df in [data_og_a0_encoded, data_big_a1_encoded]:\n",
        "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Note: Data should already be sorted chronologically from feature engineering step\n",
        "# Reset index to ensure clean indexing\n",
        "for df in [data_og_a0_encoded, data_big_a1_encoded]:\n",
        "    df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n",
        "print(f\"A0 shape: {data_og_a0_encoded.shape}\")\n",
        "print(f\"A1 shape: {data_big_a1_encoded.shape}\")\n",
        "\n",
        "# Check for non-numeric columns\n",
        "non_numeric_a0 = data_og_a0_encoded.select_dtypes(exclude=[np.number]).columns\n",
        "non_numeric_a1 = data_big_a1_encoded.select_dtypes(exclude=[np.number]).columns\n",
        "print(f\"\\nNon-numeric columns in A0: {list(non_numeric_a0)}\")\n",
        "print(f\"Non-numeric columns in A1: {list(non_numeric_a1)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A0 - Train: (34074, 25), Test: (8519, 25)\n",
            "A1 - Train: (34036, 31), Test: (8509, 31)\n"
          ]
        }
      ],
      "source": [
        "# Chronological train/test split (80/20)\n",
        "# Since data is already sorted chronologically from feature engineering, we split by index\n",
        "\n",
        "def chrono_split(df, test_frac=0.2):\n",
        "    \"\"\"Split dataframe chronologically (assumes data is already sorted by date).\"\"\"\n",
        "    df = df.reset_index(drop=True)\n",
        "    cut = int(np.floor((1 - test_frac) * len(df)))\n",
        "    return df.iloc[:cut].copy(), df.iloc[cut:].copy()\n",
        "\n",
        "# Split A0 dataset\n",
        "train0, test0 = chrono_split(data_og_a0_encoded)\n",
        "X_train_0 = train0.drop(columns='Target')\n",
        "y_train_0 = train0['Target'].astype(int)\n",
        "X_test_0 = test0.drop(columns='Target')\n",
        "y_test_0 = test0['Target'].astype(int)\n",
        "\n",
        "# Split A1 dataset\n",
        "train1, test1 = chrono_split(data_big_a1_encoded)\n",
        "X_train_1 = train1.drop(columns='Target')\n",
        "y_train_1 = train1['Target'].astype(int)\n",
        "X_test_1 = test1.drop(columns='Target')\n",
        "y_test_1 = test1['Target'].astype(int)\n",
        "\n",
        "print(f\"A0 - Train: {X_train_0.shape}, Test: {X_test_0.shape}\")\n",
        "print(f\"A1 - Train: {X_train_1.shape}, Test: {X_test_1.shape}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Preprocessing complete.\n",
            "A0 processed - Train: (34074, 25), Test: (8519, 25)\n",
            "A1 processed - Train: (34036, 31), Test: (8509, 31)\n"
          ]
        }
      ],
      "source": [
        "# Build preprocessing pipeline\n",
        "def build_preprocessor(X, for_linear=False, use_knn=True):\n",
        "    \"\"\"Build preprocessing pipeline with imputation.\"\"\"\n",
        "    num_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    cat_cols = X.select_dtypes(include=[\"object\", \"category\", \"bool\"]).columns.tolist()\n",
        "    \n",
        "    num_steps = [(\"impute_num\", KNNImputer(n_neighbors=5))] if use_knn else \\\n",
        "                [(\"impute_num\", SimpleImputer(strategy=\"median\"))]\n",
        "    if for_linear:\n",
        "        num_steps.append((\"scale\", StandardScaler()))\n",
        "    \n",
        "    num_pipe = Pipeline(num_steps)\n",
        "    cat_pipe = Pipeline([\n",
        "        (\"impute_cat\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"ohe\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True)),\n",
        "    ])\n",
        "    \n",
        "    pre = ColumnTransformer(\n",
        "        [(\"num\", num_pipe, num_cols),\n",
        "         (\"cat\", cat_pipe, cat_cols)],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "    return pre\n",
        "\n",
        "# Create preprocessors for A0 and A1\n",
        "pre0 = build_preprocessor(X_train_0, for_linear=False, use_knn=True)\n",
        "pre1 = build_preprocessor(X_train_1, for_linear=False, use_knn=True)\n",
        "\n",
        "# Fit preprocessors and transform data\n",
        "X_train_0_processed = pre0.fit_transform(X_train_0)\n",
        "X_test_0_processed = pre0.transform(X_test_0)\n",
        "X_train_1_processed = pre1.fit_transform(X_train_1)\n",
        "X_test_1_processed = pre1.transform(X_test_1)\n",
        "\n",
        "print(\"Preprocessing complete.\")\n",
        "print(f\"A0 processed - Train: {X_train_0_processed.shape}, Test: {X_test_0_processed.shape}\")\n",
        "print(f\"A1 processed - Train: {X_train_1_processed.shape}, Test: {X_test_1_processed.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Model Creation and Training\n",
        "\n",
        "We train three types of models for both A0 and A1 datasets:\n",
        "- Random Forest Classifier\n",
        "- XGBoost Classifier\n",
        "- Logistic Regression\n",
        "\n",
        "We use TimeSeriesSplit for cross-validation to prevent data leakage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Time series cross-validation\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Convert sparse matrices to dense if needed (for some models)\n",
        "from scipy.sparse import issparse\n",
        "if issparse(X_train_0_processed):\n",
        "    X_train_0_processed = X_train_0_processed.toarray()\n",
        "    X_test_0_processed = X_test_0_processed.toarray()\n",
        "if issparse(X_train_1_processed):\n",
        "    X_train_1_processed = X_train_1_processed.toarray()\n",
        "    X_test_1_processed = X_test_1_processed.toarray()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.1 A0 Models\n",
        "\n",
        "#### Random Forest Classifier (A0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest A0 trained successfully.\n"
          ]
        }
      ],
      "source": [
        "# Random Forest A0 - Using pre-computed best parameters\n",
        "# Best parameters from grid search:\n",
        "# {'criterion': 'log_loss', 'max_depth': 14, 'min_samples_leaf': 61, 'min_samples_split': 282}\n",
        "\n",
        "rf_a0 = RandomForestClassifier(\n",
        "    n_estimators=600,\n",
        "    criterion='log_loss',\n",
        "    max_depth=14,\n",
        "    min_samples_leaf=61,\n",
        "    min_samples_split=282,\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "rf_a0.fit(X_train_0_processed, y_train_0)\n",
        "pred_rf_a0 = rf_a0.predict(X_test_0_processed)\n",
        "\n",
        "print(\"Random Forest A0 trained successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### XGBoost Classifier (A0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost A0 trained successfully.\n"
          ]
        }
      ],
      "source": [
        "# XGBoost A0 - Using pre-computed best parameters\n",
        "# Best parameters from grid search:\n",
        "# {'colsample_bytree': 0.8061000499878099, 'learning_rate': 0.021098111168009435, \n",
        "#  'max_depth': 5, 'n_estimators': 154, 'subsample': 0.9044486520109609}\n",
        "\n",
        "xgb_a0 = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    n_estimators=154,\n",
        "    max_depth=5,\n",
        "    learning_rate=0.021098111168009435,\n",
        "    subsample=0.9044486520109609,\n",
        "    colsample_bytree=0.8061000499878099,\n",
        "    tree_method='hist',\n",
        "    random_state=RANDOM_STATE,\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "xgb_a0.fit(X_train_0_processed, y_train_0)\n",
        "pred_xgb_a0 = xgb_a0.predict(X_test_0_processed)\n",
        "\n",
        "print(\"XGBoost A0 trained successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic Regression (A0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression A0 trained successfully.\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression A0 with time series cross-validation\n",
        "logreg_a0 = LogisticRegressionCV(\n",
        "    max_iter=10000,\n",
        "    cv=tscv,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "logreg_a0.fit(X_train_0_processed, y_train_0)\n",
        "pred_logreg_a0 = logreg_a0.predict(X_test_0_processed)\n",
        "\n",
        "print(\"Logistic Regression A0 trained successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6.2 A1 Models\n",
        "\n",
        "#### Random Forest Classifier (A1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest A1 trained successfully.\n"
          ]
        }
      ],
      "source": [
        "# Random Forest A1 - Using pre-computed best parameters\n",
        "# Best parameters from grid search:\n",
        "# {'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 128, 'min_samples_split': 297}\n",
        "\n",
        "rf_a1 = RandomForestClassifier(\n",
        "    n_estimators=600,\n",
        "    criterion='log_loss',\n",
        "    max_depth=15,\n",
        "    min_samples_leaf=128,\n",
        "    min_samples_split=297,\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "rf_a1.fit(X_train_1_processed, y_train_1)\n",
        "pred_rf_a1 = rf_a1.predict(X_test_1_processed)\n",
        "\n",
        "print(\"Random Forest A1 trained successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### XGBoost Classifier (A1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "XGBoost A1 trained successfully.\n"
          ]
        }
      ],
      "source": [
        "# XGBoost A1 - Using pre-computed best parameters\n",
        "# Best parameters from grid search:\n",
        "# {'colsample_bytree': 0.9048980247717036, 'learning_rate': 0.03257244251360208,\n",
        "#  'max_depth': 4, 'n_estimators': 32, 'subsample': 0.8405358828849236}\n",
        "\n",
        "xgb_a1 = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    n_estimators=32,\n",
        "    max_depth=4,\n",
        "    learning_rate=0.03257244251360208,\n",
        "    subsample=0.8405358828849236,\n",
        "    colsample_bytree=0.9048980247717036,\n",
        "    tree_method='hist',\n",
        "    random_state=RANDOM_STATE,\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "xgb_a1.fit(X_train_1_processed, y_train_1)\n",
        "pred_xgb_a1 = xgb_a1.predict(X_test_1_processed)\n",
        "\n",
        "print(\"XGBoost A1 trained successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Logistic Regression (A1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logistic Regression A1 trained successfully.\n"
          ]
        }
      ],
      "source": [
        "# Logistic Regression A1 with time series cross-validation\n",
        "logreg_a1 = LogisticRegressionCV(\n",
        "    max_iter=10000,\n",
        "    cv=tscv,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "logreg_a1.fit(X_train_1_processed, y_train_1)\n",
        "pred_logreg_a1 = logreg_a1.predict(X_test_1_processed)\n",
        "\n",
        "print(\"Logistic Regression A1 trained successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Evaluation\n",
        "\n",
        "We evaluate all models using multiple metrics:\n",
        "- Accuracy (with 95% confidence intervals)\n",
        "- F1-score\n",
        "- Precision-Recall (Average Precision)\n",
        "- ROC-AUC\n",
        "- Confusion matrices\n",
        "- Threshold optimization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def best_thr_by_f1(y_true, prob):\n",
        "    \"\"\"Find best threshold by maximizing F1 score.\"\"\"\n",
        "    p, r, t = precision_recall_curve(y_true, prob)\n",
        "    f1 = 2 * p * r / (p + r + 1e-9)\n",
        "    i = np.nanargmax(f1)\n",
        "    return float(t[i]) if i < len(t) else 0.5\n",
        "\n",
        "def acc_ci(y_true, y_pred, B=1000, seed=0):\n",
        "    \"\"\"Calculate 95% confidence interval for accuracy using bootstrap.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    n = len(y_true)\n",
        "    vals = []\n",
        "    for _ in range(B):\n",
        "        idx = rng.integers(0, n, size=n)\n",
        "        vals.append(accuracy_score(y_true[idx], y_pred[idx]))\n",
        "    return tuple(np.percentile(vals, [2.5, 97.5]))\n",
        "\n",
        "def fit_and_eval(model, Xtr, ytr, Xte, yte, label):\n",
        "    \"\"\"Train model and evaluate with threshold optimization.\"\"\"\n",
        "    # Get probabilities\n",
        "    pr_tr = model.predict_proba(Xtr)[:, 1]\n",
        "    thr = best_thr_by_f1(ytr.values, pr_tr)\n",
        "    \n",
        "    pr_te = model.predict_proba(Xte)[:, 1]\n",
        "    pred = (pr_te >= thr).astype(int)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    acc = accuracy_score(yte, pred)\n",
        "    f1 = f1_score(yte, pred)\n",
        "    ap = average_precision_score(yte, pr_te)\n",
        "    auc = roc_auc_score(yte, pr_te)\n",
        "    cm = confusion_matrix(yte, pred, labels=[0, 1])\n",
        "    lo, hi = acc_ci(yte.values, pred)\n",
        "    \n",
        "    print(f\"{label}: thr={thr:.3f} | ACC={acc:.4f} (95% [{lo:.4f},{hi:.4f}]) | \"\n",
        "          f\"F1={f1:.4f} | AP={ap:.4f} | AUC={auc:.4f}\")\n",
        "    print(\"Confusion matrix [TN FP; FN TP]:\")\n",
        "    print(cm, \"\\n\")\n",
        "    \n",
        "    return {\n",
        "        'label': label, 'thr': thr, 'acc': acc, 'f1': f1, \n",
        "        'ap': ap, 'auc': auc, 'cm': cm, 'pred': pred, 'proba': pr_te\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== A0 Models ===\n",
            "RF A0: thr=0.431 | ACC=0.5198 (95% [0.5106,0.5304]) | F1=0.6681 | AP=0.5746 | AUC=0.5570\n",
            "Confusion matrix [TN FP; FN TP]:\n",
            "[[ 310 3805]\n",
            " [ 286 4118]] \n",
            "\n",
            "XGB A0: thr=0.406 | ACC=0.5222 (95% [0.5123,0.5330]) | F1=0.6753 | AP=0.5726 | AUC=0.5572\n",
            "Confusion matrix [TN FP; FN TP]:\n",
            "[[ 216 3899]\n",
            " [ 171 4233]] \n",
            "\n",
            "LogReg A0: thr=0.360 | ACC=0.5168 (95% [0.5069,0.5280]) | F1=0.6808 | AP=0.5699 | AUC=0.5537\n",
            "Confusion matrix [TN FP; FN TP]:\n",
            "[[  14 4101]\n",
            " [  15 4389]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate A0 models\n",
        "print(\"=== A0 Models ===\")\n",
        "res_rf_a0 = fit_and_eval(rf_a0, X_train_0_processed, y_train_0, X_test_0_processed, y_test_0, \"RF A0\")\n",
        "res_xgb_a0 = fit_and_eval(xgb_a0, X_train_0_processed, y_train_0, X_test_0_processed, y_test_0, \"XGB A0\")\n",
        "res_logreg_a0 = fit_and_eval(logreg_a0, X_train_0_processed, y_train_0, X_test_0_processed, y_test_0, \"LogReg A0\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== A1 Models ===\n",
            "RF A1: thr=0.396 | ACC=0.5306 (95% [0.5199,0.5411]) | F1=0.6749 | AP=0.6145 | AUC=0.5987\n",
            "Confusion matrix [TN FP; FN TP]:\n",
            "[[ 370 3739]\n",
            " [ 255 4145]] \n",
            "\n",
            "XGB A1: thr=0.407 | ACC=0.5246 (95% [0.5135,0.5345]) | F1=0.6813 | AP=0.6176 | AUC=0.6027\n",
            "Confusion matrix [TN FP; FN TP]:\n",
            "[[ 141 3968]\n",
            " [  77 4323]] \n",
            "\n",
            "LogReg A1: thr=0.330 | ACC=0.5234 (95% [0.5123,0.5329]) | F1=0.6816 | AP=0.6161 | AUC=0.6019\n",
            "Confusion matrix [TN FP; FN TP]:\n",
            "[[ 113 3996]\n",
            " [  59 4341]] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Evaluate A1 models\n",
        "print(\"=== A1 Models ===\")\n",
        "res_rf_a1 = fit_and_eval(rf_a1, X_train_1_processed, y_train_1, X_test_1_processed, y_test_1, \"RF A1\")\n",
        "res_xgb_a1 = fit_and_eval(xgb_a1, X_train_1_processed, y_train_1, X_test_1_processed, y_test_1, \"XGB A1\")\n",
        "res_logreg_a1 = fit_and_eval(logreg_a1, X_train_1_processed, y_train_1, X_test_1_processed, y_test_1, \"LogReg A1\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.1 Per-Country Accuracy Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Per-Country Accuracy Comparison:\n",
            "  Country  accuracy_A0  accuracy_A1      lift\n",
            "6       P     0.586022     0.633423  0.047402\n",
            "1       E     0.516744     0.561069  0.044324\n",
            "9       T     0.528436     0.568720  0.040284\n",
            "8      SP     0.539589     0.577713  0.038123\n",
            "0       D     0.572573     0.603217  0.030644\n",
            "4       I     0.537797     0.562635  0.024838\n",
            "2       F     0.533069     0.553642  0.020574\n",
            "5       N     0.590296     0.607046  0.016750\n",
            "3       G     0.530686     0.537906  0.007220\n",
            "7      SC     0.522472     0.521935 -0.000537\n"
          ]
        }
      ],
      "source": [
        "def per_country_acc(fitted_model, Xte, yte, preprocessor):\n",
        "    \"\"\"Calculate per-country accuracy.\"\"\"\n",
        "    # Get country information from dummies\n",
        "    country_cols = [c for c in Xte.columns if c.startswith(\"Country_\")]\n",
        "    if not country_cols:\n",
        "        print(\"No Country info present.\")\n",
        "        return None\n",
        "    \n",
        "    # Transform test data\n",
        "    Xte_processed = preprocessor.transform(Xte)\n",
        "    if issparse(Xte_processed):\n",
        "        Xte_processed = Xte_processed.toarray()\n",
        "    \n",
        "    # Get country labels\n",
        "    country = Xte[country_cols].idxmax(axis=1).str.replace(\"Country_\", \"\", regex=False)\n",
        "    \n",
        "    # Get predictions\n",
        "    probs = fitted_model.predict_proba(Xte_processed)[:, 1]\n",
        "    pred = (probs >= 0.5).astype(int)\n",
        "    \n",
        "    # Calculate per-country accuracy\n",
        "    tmp = pd.DataFrame({\"Country\": country, \"y\": yte.values, \"p\": pred})\n",
        "    return tmp.groupby(\"Country\").apply(lambda g: accuracy_score(g[\"y\"], g[\"p\"]))\\\n",
        "              .rename(\"accuracy\").reset_index()\n",
        "\n",
        "# Calculate per-country accuracy for best models (using Random Forest)\n",
        "pc_rf_a0 = per_country_acc(rf_a0, X_test_0, y_test_0, pre0)\n",
        "pc_rf_a1 = per_country_acc(rf_a1, X_test_1, y_test_1, pre1)\n",
        "\n",
        "if pc_rf_a0 is not None and pc_rf_a1 is not None:\n",
        "    pc = pc_rf_a0.merge(pc_rf_a1, on=\"Country\", suffixes=(\"_A0\", \"_A1\"))\n",
        "    pc[\"lift\"] = pc[\"accuracy_A1\"] - pc[\"accuracy_A0\"]\n",
        "    print(\"\\nPer-Country Accuracy Comparison:\")\n",
        "    print(pc.sort_values(\"lift\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 7.2 Profit Calculations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Margin Decomposition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "m = m_\\text{profit} + m_\\text{operations} + m_\\text{risk}(A)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "m_\\text{risk}(A) = k \\cdot (1 - A)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Demand Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "V(m) = \\alpha \\cdot m^{-\\epsilon}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Profit Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\Pi(m_\\text{profit}) = V(m) \\cdot m_\\text{profit} \\cdot b = \\alpha \\cdot m^{-\\epsilon} \\cdot m_\\text{profit} \\cdot b\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Optimal Profit Margin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We take the derivative and set it to zero to find the value of the decision variable `m_profit` that gives the highest (maximum) profit."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "[\n",
        "\\frac{d\\Pi}{dm_\\text{profit}} = 0\n",
        "]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lats denote:\n",
        "$$\n",
        "C = m_\\text{operations} + m_\\text{risk}(A)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\frac{\\mathrm{d}}{\\mathrm{d}m_{profit}}\\left[{\\alpha} \\left(m_{profit} + C\\right)^{-{\\epsilon}} m_{profit} b\\right] = 0\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "m_\\text{profit}^* = \\frac{C}{\\epsilon - 1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Kudos to https://www.derivative-calculator.net/ for helping with derivative calculations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Optimal total margin:\n",
        "$$\n",
        "m^* = m_\\text{profit}^* + C = \\frac{\\epsilon \\, C}{\\epsilon - 1}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Maximum Profit at Optimum"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "$$\n",
        "\\Pi^* = \\alpha \\cdot \\left(\\frac{\\epsilon}{\\epsilon-1} C \\right)^{-\\epsilon} \\cdot \\frac{C}{\\epsilon-1} \\cdot b\n",
        "$$\n",
        "then\n",
        "$$\n",
        "\\Pi^* = \\alpha \\, b \\cdot \\frac{1}{\\epsilon-1} \\cdot \\left(\\frac{\\epsilon}{\\epsilon-1} \\right)^{-\\epsilon} C^{1-\\epsilon}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "For ( \\epsilon = 3 ):\n",
        "\\Pi^* = \\alpha \\, b \\cdot \\frac{4}{27} \\cdot C^{-2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Profit Increase from Increase in Accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "$$\n",
        "\\Delta\\Pi^* = \\Pi^*\\left(A_1\\right) - \\Pi^*\\left(A_0\\right)\n",
        "$$\n",
        "where\n",
        "$$\n",
        "\\Pi^*(A) = \\frac{4 \\alpha b}{27} \\cdot [C(A)]^{-2}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A0 Accuracy: 0.5198\n",
            "A1 Accuracy: 0.5306\n",
            "\n",
            "Overall Î”Profit (USD/yr): 2254.38\n",
            "\n",
            "Per-Country Profit Comparison:\n",
            "  Country  accuracy_A0  accuracy_A1  DeltaPi_USDyr\n",
            "6       P     0.586022     0.633423   15964.838849\n",
            "1       E     0.516744     0.561069    9943.918730\n",
            "9       T     0.528436     0.568720    9517.203214\n",
            "8      SP     0.539589     0.577713    9516.988694\n",
            "0       D     0.572573     0.603217    9030.414901\n",
            "4       I     0.537797     0.562635    5910.834438\n",
            "5       N     0.590296     0.607046    5254.645432\n",
            "2       F     0.533069     0.553642    4715.615487\n",
            "3       G     0.530686     0.537906    1575.733898\n",
            "7      SC     0.522472     0.521935    -110.011279\n"
          ]
        }
      ],
      "source": [
        "# Constants from assignment\n",
        "m_operations = 0.03\n",
        "k = 0.3\n",
        "alpha = 1000\n",
        "epsilon = 3\n",
        "b = 12.0\n",
        "\n",
        "# Margin decomposition: m = m_profit + m_operations + m_risk(A)\n",
        "# where:\n",
        "# - m_profit: adjustable profit component\n",
        "# - m_operations: fixed operational cost margin\n",
        "# - m_risk(A): risk margin that depends on accuracy A\n",
        "\n",
        "def m_risk(A):\n",
        "    \"\"\"Risk margin function: m_risk(A) = k * (1 - A)\"\"\"\n",
        "    return k * (1.0 - A)\n",
        "\n",
        "def C(A):\n",
        "    \"\"\"Cost function: C(A) = m_operations + m_risk(A)\n",
        "    Represents the fixed cost components that don't depend on m_profit.\n",
        "    \"\"\"\n",
        "    return m_operations + m_risk(A)\n",
        "\n",
        "def profit_star(A):\n",
        "    \"\"\"Optimal profit function for epsilon=3.\n",
        "    \n",
        "    This is the result of optimizing Î (m_profit) = V(m) * m_profit * b\n",
        "    where:\n",
        "    - V(m) = Î± * m^(-Îµ) is the demand function\n",
        "    - m = m_profit + C(A) is the total margin\n",
        "    - The optimal m_profit is found by maximizing Î  with respect to m_profit\n",
        "    \n",
        "    For epsilon=3, the optimized profit is: (4 * Î± * b / 27) * C(A)^(-2)\n",
        "    \"\"\"\n",
        "    return (4 * alpha * b / 27.0) * (C(A) ** -2)\n",
        "\n",
        "# Calculate profit for best models (using Random Forest)\n",
        "A0_acc = res_rf_a0[\"acc\"]\n",
        "A1_acc = res_rf_a1[\"acc\"]\n",
        "\n",
        "print(f\"A0 Accuracy: {A0_acc:.4f}\")\n",
        "print(f\"A1 Accuracy: {A1_acc:.4f}\")\n",
        "print(f\"\\nOverall Î”Profit (USD/yr): {profit_star(A1_acc) - profit_star(A0_acc):.2f}\")\n",
        "\n",
        "# Per-country profit (if available)\n",
        "if pc_rf_a0 is not None and pc_rf_a1 is not None:\n",
        "    pc_val = pc_rf_a0.merge(pc_rf_a1, on=\"Country\", suffixes=(\"_A0\", \"_A1\")).copy()\n",
        "    pc_val[\"DeltaPi_USDyr\"] = profit_star(pc_val[\"accuracy_A1\"]) - profit_star(pc_val[\"accuracy_A0\"])\n",
        "    print(\"\\nPer-Country Profit Comparison:\")\n",
        "    print(pc_val.sort_values(\"DeltaPi_USDyr\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Results and Conclusions\n",
        "\n",
        "### Summary of Results\n",
        "\n",
        "The models have been trained and evaluated on both A0 (limited features) and A1 (extended features with betting odds) datasets. Key findings:\n",
        "\n",
        "1. **Model Performance**: All models show similar performance, with slight variations between A0 and A1 datasets.\n",
        "\n",
        "2. **Feature Impact**: The addition of betting odds features (A1) provides additional information that may improve predictions.\n",
        "\n",
        "3. **Best Models**: Random Forest and XGBoost generally perform well, with Logistic Regression providing a baseline comparison.\n",
        "\n",
        "4. **Economic Impact**: The profit calculations show the economic value of improved accuracy in the betting context.\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- Time series cross-validation is crucial to prevent data leakage\n",
        "- Feature engineering (rolling averages, market features) adds valuable information\n",
        "- Threshold optimization can improve model performance for specific use cases\n",
        "- Per-country analysis reveals variations in model performance across different leagues\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Further hyperparameter tuning could be explored\n",
        "- Feature importance analysis could identify the most predictive features\n",
        "- Ensemble methods could potentially improve performance\n",
        "- Additional feature engineering based on domain knowledge\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
