{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELyF3H50yNC_"
      },
      "source": [
        "## Author's notes\n",
        "### 28.10.2025\n",
        "#### Issues to solve\n",
        "I am starting the model creation process. The files created in the prerocessing will be used, however, I do have some issues with them.\n",
        "1) The data has not been split into train and test set during the preprocessing phase. I am aware that label encoding of categorical variables has been done and we should know all the categories for the teams, but it is still best practice to do encoding AFTER the split in order to avoid data leakage and it may be a problem in the assessment of our work by the teachers. Therefore, it would probably be for the best if this was corrected. All the preprocessing steps should be fitted to only the train data and only then used to transform the test data.\n",
        "2) Only the market values were used in the bigger dataset. I am not saying this is wrong or right and I will trust Vojta on this one, BUT there should be a very detailed explanation for why exactly we didn't use the rest of the data.\n",
        "3) There are missing values in the first observations in the derived column. I have dropped them for now, but I think a KNN imputer might do the trick, we could do some imputations and use everything.\n",
        "\n",
        "Because of the issues with the train test split, I will split the data here, but I would like to change it once it is fixed.\n",
        "\n",
        "#### A0 models\n",
        "\n",
        "The models for dataset A0 are done, along with hyperparameter tuning done thrugh RandomizedSearchCV and GridSearchCV. The data has been split chornologically, to use only historic data to predict future data. For this reason, TimeSeriesSplit has been used as the cross-validation method for the hyperparameter tuning, in order to prevent data leakage from the future into the past.\n",
        "The models implemented so far are a random forest classifier and a tree based XGBoost classifier. Both have been tuned and the results of the grid and random searches are saved as text for convenience in future work on this project. However, it in important to note that if any changes are done on the data (like fixing the issues highlighted above), it will be necessary to recompute the results.\n",
        "A Logistic regression model has been added, a time series cross validation is also used in it.\n",
        "\n",
        "IF YOU WANT TO DO ONLY EVALUATION AND NOT TUNING, DON'T RUN THE CELLS WITH RANDOMIZED AND GRID SEARCH. USE THE COMMENT VERSION OF THE PARAMETERS THAT IS SAVED IN THE CELLS BELOW.\n",
        "\n",
        "If I remember correctly what the teachers said in the first lesson, the persformance of our model will not be much better than flipping a coin, so keep that in mind when evaluating the models, they are unlikely to be outstandingly good. Honestly, if they are too good, maybe let the rest of the team now, because there may be some data leakage happening.\n",
        "\n",
        "To enhance the performance, maybe playing around with the decision threshold could help, but I will leave that up to Zahra and her tuning efforts.\n",
        "\n",
        "#### A1 models\n",
        "Random forest is kicking my ass at the moment, hopefully I can fix it somehow. Idk if the issue is with me or the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "pKBfgP1t21gI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data import, train test split and shenanigans\n",
        "The train test split should later be replaced by just loading the already split data after it has been done in preprocessing.\n",
        "After this step, the models for A0 dataset will be made in the first chunk and A1 on the second chunk of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "#data import\n",
        "FILE_PATH_a0 = \"ready_data\\data_a0_encoded.csv\"\n",
        "FILE_PATH_a1 = \"ready_data\\data_a1_encoded.csv\"\n",
        "\n",
        "data_a0=pd.read_csv(FILE_PATH_a0)\n",
        "data_a1=pd.read_csv(FILE_PATH_a1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42593 entries, 0 to 42592\n",
            "Data columns (total 26 columns):\n",
            " #   Column                         Non-Null Count  Dtype  \n",
            "---  ------                         --------------  -----  \n",
            " 0   Time                           42593 non-null  int64  \n",
            " 1   Target                         42593 non-null  int64  \n",
            " 2   HomeTeam_enc                   42593 non-null  int64  \n",
            " 3   avg_goals_in_last5_home        42115 non-null  float64\n",
            " 4   avg_goals_conceded_last5_home  42115 non-null  float64\n",
            " 5   AwayTeam_enc                   42593 non-null  int64  \n",
            " 6   avg_goals_in_last5_away        42115 non-null  float64\n",
            " 7   avg_goals_conceded_last5_away  42115 non-null  float64\n",
            " 8   Year                           42593 non-null  int64  \n",
            " 9   Month                          42593 non-null  int64  \n",
            " 10  Dayofweek                      42593 non-null  int64  \n",
            " 11  Is_weekend                     42593 non-null  int64  \n",
            " 12  Season_of_year                 42593 non-null  int64  \n",
            " 13  Country_D                      42593 non-null  bool   \n",
            " 14  Country_E                      42593 non-null  bool   \n",
            " 15  Country_F                      42593 non-null  bool   \n",
            " 16  Country_G                      42593 non-null  bool   \n",
            " 17  Country_I                      42593 non-null  bool   \n",
            " 18  Country_N                      42593 non-null  bool   \n",
            " 19  Country_P                      42593 non-null  bool   \n",
            " 20  Country_SC                     42593 non-null  bool   \n",
            " 21  Country_SP                     42593 non-null  bool   \n",
            " 22  Country_T                      42593 non-null  bool   \n",
            " 23  Division_1                     42593 non-null  bool   \n",
            " 24  Division_2                     42593 non-null  bool   \n",
            " 25  Division_3                     42593 non-null  bool   \n",
            "dtypes: bool(13), float64(4), int64(9)\n",
            "memory usage: 4.8 MB\n",
            "None\n",
            "   Time  Target  HomeTeam_enc  avg_goals_in_last5_home  \\\n",
            "0    19       1           420                      NaN   \n",
            "1    19       0           101                      NaN   \n",
            "2    19       1             8                      NaN   \n",
            "3    19       1           201                      NaN   \n",
            "4    19       0           372                      NaN   \n",
            "\n",
            "   avg_goals_conceded_last5_home  AwayTeam_enc  avg_goals_in_last5_away  \\\n",
            "0                            NaN           204                      NaN   \n",
            "1                            NaN           444                      NaN   \n",
            "2                            NaN           245                      NaN   \n",
            "3                            NaN           196                      NaN   \n",
            "4                            NaN            44                      NaN   \n",
            "\n",
            "   avg_goals_conceded_last5_away  Year  Month  ...  Country_G  Country_I  \\\n",
            "0                            NaN  2019      7  ...      False      False   \n",
            "1                            NaN  2019      7  ...      False      False   \n",
            "2                            NaN  2019      7  ...      False      False   \n",
            "3                            NaN  2019      7  ...      False      False   \n",
            "4                            NaN  2019      7  ...      False      False   \n",
            "\n",
            "   Country_N  Country_P  Country_SC  Country_SP  Country_T  Division_1  \\\n",
            "0      False      False       False       False      False       False   \n",
            "1      False      False       False       False      False       False   \n",
            "2      False      False       False       False      False       False   \n",
            "3      False      False       False       False      False       False   \n",
            "4      False      False       False       False      False       False   \n",
            "\n",
            "   Division_2  Division_3  \n",
            "0        True       False  \n",
            "1        True       False  \n",
            "2        True       False  \n",
            "3        True       False  \n",
            "4        True       False  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42593 entries, 0 to 42592\n",
            "Data columns (total 32 columns):\n",
            " #   Column                         Non-Null Count  Dtype  \n",
            "---  ------                         --------------  -----  \n",
            " 0   Time                           42593 non-null  int64  \n",
            " 1   Target                         42593 non-null  int64  \n",
            " 2   HomeTeam_enc                   42593 non-null  int64  \n",
            " 3   avg_goals_in_last5_home        42115 non-null  float64\n",
            " 4   avg_goals_conceded_last5_home  42115 non-null  float64\n",
            " 5   AwayTeam_enc                   42593 non-null  int64  \n",
            " 6   avg_goals_in_last5_away        42115 non-null  float64\n",
            " 7   avg_goals_conceded_last5_away  42115 non-null  float64\n",
            " 8   market_decisiveness            42546 non-null  float64\n",
            " 9   expected_total_goals           42546 non-null  float64\n",
            " 10  Norm_Ah_P_home                 42546 non-null  float64\n",
            " 11  Norm_Ah_P_away                 42546 non-null  float64\n",
            " 12  ah_imbalance                   42546 non-null  float64\n",
            " 13  ah_market_confidence           42546 non-null  float64\n",
            " 14  Year                           42593 non-null  int64  \n",
            " 15  Month                          42593 non-null  int64  \n",
            " 16  Dayofweek                      42593 non-null  int64  \n",
            " 17  Is_weekend                     42593 non-null  int64  \n",
            " 18  Season_of_year                 42593 non-null  int64  \n",
            " 19  Country_D                      42593 non-null  bool   \n",
            " 20  Country_E                      42593 non-null  bool   \n",
            " 21  Country_F                      42593 non-null  bool   \n",
            " 22  Country_G                      42593 non-null  bool   \n",
            " 23  Country_I                      42593 non-null  bool   \n",
            " 24  Country_N                      42593 non-null  bool   \n",
            " 25  Country_P                      42593 non-null  bool   \n",
            " 26  Country_SC                     42593 non-null  bool   \n",
            " 27  Country_SP                     42593 non-null  bool   \n",
            " 28  Country_T                      42593 non-null  bool   \n",
            " 29  Division_1                     42593 non-null  bool   \n",
            " 30  Division_2                     42593 non-null  bool   \n",
            " 31  Division_3                     42593 non-null  bool   \n",
            "dtypes: bool(13), float64(10), int64(9)\n",
            "memory usage: 6.7 MB\n",
            "None\n",
            "   Time  Target  HomeTeam_enc  avg_goals_in_last5_home  \\\n",
            "0    19       0           307                      NaN   \n",
            "1    19       1           184                      NaN   \n",
            "2    19       0           372                      NaN   \n",
            "3    19       1           420                      NaN   \n",
            "4    19       1           201                      NaN   \n",
            "\n",
            "   avg_goals_conceded_last5_home  AwayTeam_enc  avg_goals_in_last5_away  \\\n",
            "0                            NaN           435                      1.4   \n",
            "1                            NaN           238                      1.0   \n",
            "2                            NaN            44                      1.4   \n",
            "3                            NaN           204                      1.2   \n",
            "4                            NaN           196                      0.8   \n",
            "\n",
            "   avg_goals_conceded_last5_away  market_decisiveness  expected_total_goals  \\\n",
            "0                            2.0             0.093350              1.677022   \n",
            "1                            1.2             0.121554              1.958666   \n",
            "2                            2.2             0.114610              1.644545   \n",
            "3                            0.6             0.066234              1.890909   \n",
            "4                            0.8             0.070312              1.710794   \n",
            "\n",
            "   ...  Country_G  Country_I  Country_N  Country_P  Country_SC  Country_SP  \\\n",
            "0  ...      False      False      False      False       False       False   \n",
            "1  ...      False      False      False      False       False       False   \n",
            "2  ...      False      False      False      False       False       False   \n",
            "3  ...      False      False      False      False       False       False   \n",
            "4  ...      False      False      False      False       False       False   \n",
            "\n",
            "   Country_T  Division_1  Division_2  Division_3  \n",
            "0      False       False        True       False  \n",
            "1      False        True       False       False  \n",
            "2      False       False        True       False  \n",
            "3      False       False        True       False  \n",
            "4      False       False        True       False  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nThe averaged data of the last 5 games contains missing values. \\nThis is because for the first 5 matches, it is always impossible to compute the average.\\nBecause this is a derived column, this makes sense and should not be an issue for the data.\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check that everything checks out\n",
        "print(data_a0.info())\n",
        "print(data_a0.head())\n",
        "\n",
        "print(data_a1.info())\n",
        "print(data_a1.head())\n",
        "\n",
        "'''\n",
        "The averaged data of the last 5 games contains missing values. \n",
        "This is because for the first 5 matches, it is always impossible to compute the average.\n",
        "Because this is a derived column, this makes sense and should not be an issue for the data.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The country and division dummies are booleans, change that into numerical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_a0[data_a0.select_dtypes(include='bool').columns]=data_a0[data_a0.select_dtypes(include='bool').columns].astype(int)\n",
        "data_a1[data_a1.select_dtypes(include='bool').columns]=data_a1[data_a1.select_dtypes(include='bool').columns].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check that all columns have only numerical values\n",
        "non_numeric_cols0 = data_a0.select_dtypes(exclude=[np.number]).columns\n",
        "non_numeric_cols1 = data_a1.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "assert len(non_numeric_cols0)==0\n",
        "assert len(non_numeric_cols1)==0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data has been sorted chronologically in the preprocessing phase. Because this data is a time series and is likely time dependend, we will not be doing a random split of the data, but rather, a chronological one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "#just a check to see that we are good to keep working with the data and it's in the form we want\n",
        "assert type(data_a0)==pd.core.frame.DataFrame\n",
        "assert type(data_a1)==pd.core.frame.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train test split, 80/20 ratio\n",
        "#for A0\n",
        "split_index_0 = int(0.8 * len(data_a0))\n",
        "\n",
        "train0 = data_a0.iloc[:split_index_0]\n",
        "test0  = data_a0.iloc[split_index_0:]\n",
        "\n",
        "X_train_0, y_train_0 = train0.drop(columns='Target'), train0['Target']\n",
        "X_test_0,  y_test_0  = test0.drop(columns='Target'),  test0['Target']\n",
        "\n",
        "#for A1\n",
        "split_index_1 = int(0.8 * len(data_a1))\n",
        "\n",
        "train1 = data_a1.iloc[:split_index_1]\n",
        "test1  = data_a1.iloc[split_index_1:]\n",
        "\n",
        "X_train_1, y_train_1 = train1.drop(columns='Target'), train1['Target']\n",
        "X_test_1,  y_test_1  = test1.drop(columns='Target'),  test1['Target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model creation A0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RandomForestClassifier\n",
        "Because this is a classification task and we aren't looking at a continuous target variable, we will use the RandomForestClassifier and not the RandomForestRegressor we have used in the lectures.\n",
        "\n",
        "Because we have derived the avg variables, there is some missingness in the data. I will drop the observations with missing values for now but I think it can be fixed (check author's notes at the top of this markdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#temporary solution to missing values\n",
        "X_train_0 = X_train_0.dropna()\n",
        "y_train_0 = y_train_0.loc[X_train_0.index]\n",
        "\n",
        "X_test_0 = X_test_0.dropna()\n",
        "y_test_0 = y_test_0.loc[X_test_0.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "#initiate the model\n",
        "rf0_1 = RandomForestClassifier(random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#because the data is chronological, we cannot do a randomized cross-validation when choosing the model\n",
        "#we will use a rolling cross-validation instead\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=5) \n",
        "\n",
        "#this will split out data into 5 folds and we will always evaluate only based on the past, preventing leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#random search for some good values to use in gridsearch (hyperparameter tuning)\n",
        "#parameters to go through\n",
        "param_grid= {\n",
        "    'max_depth':[i for i in range(1, 30)],\n",
        "    'min_samples_split':[i for i in range(1,300)],\n",
        "    'min_samples_leaf':[i for i in range(1, 200)],\n",
        "    'criterion' :['gini', 'entropy', 'log_loss']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'max_depth': [18, 16, 14, 18], 'min_samples_split': [268, 239, 291, 282], 'min_samples_leaf': [58, 43, 61, 66], 'criterion': ['entropy', 'entropy', 'gini', 'log_loss']}\n"
          ]
        }
      ],
      "source": [
        "#the random search\n",
        "params={\n",
        "    'max_depth':[],\n",
        "    'min_samples_split':[],\n",
        "    'min_samples_leaf':[],\n",
        "    'criterion' :[]\n",
        "} #empty parameter grid to input the results of the random search\n",
        "for state in [1, 20, 42, 200]: \n",
        "    #the loop numbers were randomly chosen, they are just here as random seeds to make the results reproducible\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=rf0_1,\n",
        "        param_distributions=param_grid,\n",
        "        cv=tscv,\n",
        "        n_iter=100,\n",
        "        random_state=state,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train_0, y_train_0)\n",
        "    new_params=random_search.best_params_\n",
        "    for key, value in new_params.items():\n",
        "        params[key].append(value)\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'max_depth': [16, 18, 14], 'min_samples_split': [282, 291, 268, 239], 'min_samples_leaf': [66, 58, 43, 61], 'criterion': ['log_loss', 'gini', 'entropy']}\n"
          ]
        }
      ],
      "source": [
        "#keeping only unique values in the parameter grid\n",
        "for key in params:\n",
        "    params[key] = list(set(params[key]))\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The random search is run multiple times to find some values that could be used in the grid search. As this takes some time, here are the values that it gave me when I ran the code (so they can be used immediately and without running the code):\n",
        "\n",
        "params={'max_depth': [16, 18, 14],'min_samples_split': [282, 291, 268, 239], 'min_samples_leaf': [66, 58, 43, 61], 'criterion': ['log_loss', 'gini', 'entropy']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nparams={'max_depth': [16, 18, 14],\\n        'min_samples_split': [282, 291, 268, 239],\\n        'min_samples_leaf': [66, 58, 43, 61],\\n        'criterion': ['log_loss', 'gini', 'entropy']}\\n\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#for convenience, the params output can be loaded here\n",
        "'''\n",
        "params={'max_depth': [16, 18, 14],\n",
        "        'min_samples_split': [282, 291, 268, 239],\n",
        "        'min_samples_leaf': [66, 58, 43, 61],\n",
        "        'criterion': ['log_loss', 'gini', 'entropy']}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'criterion': 'log_loss', 'max_depth': 14, 'min_samples_leaf': 61, 'min_samples_split': 282}\n"
          ]
        }
      ],
      "source": [
        "#grid search\n",
        "grid=GridSearchCV(estimator=rf0_1,\n",
        "                  param_grid=params,\n",
        "                  n_jobs=-1, \n",
        "                  cv=tscv)\n",
        "grid.fit(X_train_0, y_train_0)\n",
        "\n",
        "best_params=grid.best_params_\n",
        "print(best_params)\n",
        "\n",
        "rf_0=grid.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As grid search takes a lot of time to be computed, here are the hyperparameter values for the best estimator for future convenience.\n",
        "\n",
        "best_params={'criterion': 'log_loss', 'max_depth': 14, 'min_samples_leaf': 61, 'min_samples_split': 282}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for convenience, the best estimator can be loaded here\n",
        "'''\n",
        "best_params={'criterion': 'log_loss', 'max_depth': 14, 'min_samples_leaf': 61, 'min_samples_split': 282}\n",
        "rf_0=RandomForestClassifier(best_params, random_state=42)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Random forest model predictions\n",
        "pred_rf_0=rf_0.predict(X_test_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tree boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This process is mostly analogous to the random forest, only using a different classifier and different hyperparameters (since we need different things for the two). Otherwise, the model creation has the same steps and uses the time series cross validation that has already been used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "import scipy.stats as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_parameter_grid= {\n",
        "    'n_estimators': st.randint(2, 300),\n",
        "    'max_depth': st.randint(3, 8),\n",
        "    'learning_rate': st.uniform(0.01, 0.25),\n",
        "    'subsample': st.uniform(0.8, 0.2),\n",
        "    'colsample_bytree': st.uniform(0.8, 0.2)\n",
        "} #hyperparameters for xgboost, some are from a uniform distribution because they are floats and not integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_estimators': [285, 71, 102, 154], 'max_depth': [3, 4, 5, 3], 'learning_rate': [np.float64(0.030563726508461113), np.float64(0.059119574314573674), np.float64(0.019337047187303606), np.float64(0.021098111168009435)], 'subsample': [np.float64(0.8834765592635814), np.float64(0.9215534158836575), np.float64(0.9044486520109609), np.float64(0.8412233846481453)], 'colsample_bytree': [np.float64(0.8509405508246773), np.float64(0.993252878676858), np.float64(0.8061000499878099), np.float64(0.999332288418085)]}\n"
          ]
        }
      ],
      "source": [
        "#the random search\n",
        "xgb_params={\n",
        "    'n_estimators': [],\n",
        "    'max_depth': [],\n",
        "    'learning_rate': [],\n",
        "    'subsample': [],\n",
        "    'colsample_bytree': []\n",
        "} #empty parameter grid to input the results of the random search\n",
        "for state in [1, 20, 42, 200]:\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=xgb,\n",
        "        param_distributions=xgb_parameter_grid,\n",
        "        n_iter=100,\n",
        "        cv=tscv,\n",
        "        n_jobs=-1,\n",
        "        random_state=state\n",
        "        )\n",
        "    random_search.fit(X_train_0, y_train_0)\n",
        "    new_params=random_search.best_params_\n",
        "    for key, value in new_params.items():\n",
        "        xgb_params[key].append(value)\n",
        "\n",
        "print(xgb_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "random search results:\n",
        "\n",
        "{'n_estimators': [285, 71, 102, 154], 'max_depth': [3, 4, 5, 3], 'learning_rate': [np.float64(0.030563726508461113), np.float64(0.059119574314573674), np.float64(0.019337047187303606), np.float64(0.021098111168009435)], 'subsample': [np.float64(0.8834765592635814), np.float64(0.9215534158836575), np.float64(0.9044486520109609), np.float64(0.8412233846481453)], 'colsample_bytree': [np.float64(0.8509405508246773), np.float64(0.993252878676858), np.float64(0.8061000499878099), np.float64(0.999332288418085)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for convenience, the params output can be loaded here\n",
        "'''\n",
        "xgb_params={'n_estimators': [285, 71, 102, 154], \n",
        "            'max_depth': [3, 4, 5, 3], \n",
        "            'learning_rate': [np.float64(0.030563726508461113), np.float64(0.059119574314573674), \n",
        "                              np.float64(0.019337047187303606), np.float64(0.021098111168009435)], \n",
        "            'subsample': [np.float64(0.8834765592635814), np.float64(0.9215534158836575), \n",
        "                          np.float64(0.9044486520109609), np.float64(0.8412233846481453)],\n",
        "            'colsample_bytree': [np.float64(0.8509405508246773), np.float64(0.993252878676858),\n",
        "                                  np.float64(0.8061000499878099), np.float64(0.999332288418085)]}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'colsample_bytree': np.float64(0.8061000499878099), 'learning_rate': np.float64(0.021098111168009435), 'max_depth': 5, 'n_estimators': 154, 'subsample': np.float64(0.9044486520109609)}\n"
          ]
        }
      ],
      "source": [
        "xgb_grid = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=xgb_params,\n",
        "    cv=tscv,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "xgb_grid.fit(X_train_0, y_train_0)\n",
        "\n",
        "best_params=xgb_grid.best_params_\n",
        "print(best_params)\n",
        "\n",
        "xgb_0=xgb_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best estimator parameters from GridSearchCV\n",
        "{'colsample_bytree': np.float64(0.8061000499878099), 'learning_rate': np.float64(0.021098111168009435), 'max_depth': 5, 'n_estimators': 154, 'subsample': np.float64(0.9044486520109609)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for convenience, the best estimator can be loaded here\n",
        "'''\n",
        "best_params={'colsample_bytree': np.float64(0.8061000499878099),\n",
        "             'learning_rate': np.float64(0.021098111168009435),\n",
        "             'max_depth': 5, 'n_estimators': 154, 'subsample': np.float64(0.9044486520109609)}\n",
        "xgb_0=RandomForestClassifier(best_params, random_state=42)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Random forest model predictions\n",
        "pred_xgb_0=xgb_0.predict(X_test_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression\n",
        "Because we are predicting a binary outcome, we can use a logistic regression model, as it uses the probability of the outcome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "logreg_0=LogisticRegressionCV(max_iter=10000, cv=tscv) \n",
        "#logistic regression with time series Cross Validation\n",
        "\n",
        "logreg_0.fit(X_train_0, y_train_0)\n",
        "pred_logreg=logreg_0.predict(X_test_0) #predictions for the logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model creation A1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest Classifier\n",
        "Analogous to model creation 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "#temporary solution to missing values\n",
        "X_train_1 = X_train_1.dropna()\n",
        "y_train_1 = y_train_1.loc[X_train_1.index]\n",
        "\n",
        "X_test_1 = X_test_1.dropna()\n",
        "y_test_1 = y_test_1.loc[X_test_1.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "#initiate the model\n",
        "rf1_1 = RandomForestClassifier(random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:528: FitFailedWarning: \n",
            "400 fits failed out of a total of 500.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "400 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"c:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"c:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py\", line 375, in fit\n",
            "    estimator._compute_missing_values_in_feature_mask(\n",
            "  File \"c:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py\", line 222, in _compute_missing_values_in_feature_mask\n",
            "    _assert_all_finite_element_wise(X, xp=np, allow_nan=True, **common_kwargs)\n",
            "  File \"c:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py\", line 169, in _assert_all_finite_element_wise\n",
            "    raise ValueError(msg_err)\n",
            "ValueError: Input X contains infinity or a value too large for dtype('float32').\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "c:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n",
            " nan nan nan nan nan nan nan nan nan nan]\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ValueError",
          "evalue": "Input X contains infinity or a value too large for dtype('float32').",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[50], line 17\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m42\u001b[39m, \u001b[38;5;241m200\u001b[39m]:\n\u001b[0;32m      9\u001b[0m     random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m     10\u001b[0m         estimator\u001b[38;5;241m=\u001b[39mrf1_1,\n\u001b[0;32m     11\u001b[0m         param_distributions\u001b[38;5;241m=\u001b[39mparam_grid,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     16\u001b[0m     )\n\u001b[1;32m---> 17\u001b[0m     \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     new_params\u001b[38;5;241m=\u001b[39mrandom_search\u001b[38;5;241m.\u001b[39mbest_params_\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m new_params\u001b[38;5;241m.\u001b[39mitems():\n",
            "File \u001b[1;32mc:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1062\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1060\u001b[0m refit_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1062\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbest_estimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_estimator_\u001b[38;5;241m.\u001b[39mfit(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mrouted_params\u001b[38;5;241m.\u001b[39mestimator\u001b[38;5;241m.\u001b[39mfit)\n",
            "File \u001b[1;32mc:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:375\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    369\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    370\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    373\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n\u001b[0;32m    374\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 375\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_missing_values_in_feature_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    378\u001b[0m )\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    381\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X)\n",
            "File \u001b[1;32mc:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\tree\\_classes.py:222\u001b[0m, in \u001b[0;36mBaseDecisionTree._compute_missing_values_in_feature_mask\u001b[1;34m(self, X, estimator_name)\u001b[0m\n\u001b[0;32m    218\u001b[0m     overall_sum \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(X)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(overall_sum):\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Raise a ValueError in case of the presence of an infinite element.\u001b[39;00m\n\u001b[1;32m--> 222\u001b[0m     \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcommon_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;66;03m# If the sum is not nan, then there are no missing values\u001b[39;00m\n\u001b[0;32m    225\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misnan(overall_sum):\n",
            "File \u001b[1;32mc:\\Users\\emado\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    153\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    155\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    156\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    157\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    168\u001b[0m     )\n\u001b[1;32m--> 169\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
            "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float32')."
          ]
        }
      ],
      "source": [
        "#the random search\n",
        "params={\n",
        "    'max_depth':[],\n",
        "    'min_samples_split':[],\n",
        "    'min_samples_leaf':[],\n",
        "    'criterion' :[]\n",
        "} #empty parameter grid to input the results of the random search\n",
        "for state in [1, 20, 42, 200]:\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=rf1_1,\n",
        "        param_distributions=param_grid,\n",
        "        cv=tscv,\n",
        "        n_iter=100,\n",
        "        random_state=state,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train_1, y_train_1)\n",
        "    new_params=random_search.best_params_\n",
        "    for key, value in new_params.items():\n",
        "        params[key].append(value)\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#keeping only unique values in the parameter grid\n",
        "for key in params:\n",
        "    params[key] = list(set(params[key]))\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#grid search\n",
        "grid=GridSearchCV(estimator=rf1_1,\n",
        "                  param_grid=params,\n",
        "                  n_jobs=-1, \n",
        "                  cv=tscv)\n",
        "grid.fit(X_train_1, y_train_1)\n",
        "\n",
        "best_params=grid.best_params_\n",
        "print(best_params)\n",
        "\n",
        "rf_1=grid.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tree boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
