{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELyF3H50yNC_"
      },
      "source": [
        "## Author's notes\n",
        "### 28.10.2025\n",
        "#### Issues to solve\n",
        "I am starting the model creation process. The files created in the prerocessing will be used, however, I do have some issues with them.\n",
        "1) The data has not been split into train and test set during the preprocessing phase. I am aware that label encoding of categorical variables has been done and we should know all the categories for the teams, but it is still best practice to do encoding AFTER the split in order to avoid data leakage and it may be a problem in the assessment of our work by the teachers. Therefore, it would probably be for the best if this was corrected. All the preprocessing steps should be fitted to only the train data and only then used to transform the test data.\n",
        "2) Only the market values were used in the bigger dataset. I am not saying this is wrong or right and I will trust Vojta on this one, BUT there should be a very detailed explanation for why exactly we didn't use the rest of the data.\n",
        "3) There are missing values in the first observations in the derived column. I have dropped them for now, but I think a KNN imputer might do the trick, we could do some imputations and use everything.\n",
        "4) I am having some doubts about the team encoding values because it's pretty big numbers compared to everything else. I know it's categorical but I'm wondering, whether it can mess with our results. Would like the team's opinion on that, I have no idea, just a thought.\n",
        "5) There are 2 infinity values in model A1 and it's absolutely screwing me over, needs to be fixed. Idk how it happened, I'll just throw the observations out for now, BUT it would be better to fix it somehow. Check my code for finding the infinity values so you don't have to figure it out yourselves and deal with that along with the train test split. Feel free to steal my code, much of it can be just altered to make everything work. However, note that it is a working version and I didn't deal with a lot of stuff properly, but just enough to be able to make the models.\n",
        "\n",
        "Because of the issues with the train test split, I will split the data here, but I would like to change it once it is fixed.\n",
        "\n",
        "#### A0 models\n",
        "\n",
        "The models for dataset A0 are done, along with hyperparameter tuning done thrugh RandomizedSearchCV and GridSearchCV. The data has been split chornologically, to use only historic data to predict future data. For this reason, TimeSeriesSplit has been used as the cross-validation method for the hyperparameter tuning, in order to prevent data leakage from the future into the past.\n",
        "The models implemented so far are a random forest classifier and a tree based XGBoost classifier. Both have been tuned and the results of the grid and random searches are saved as text for convenience in future work on this project. However, it in important to note that if any changes are done on the data (like fixing the issues highlighted above), it will be necessary to recompute the results.\n",
        "A Logistic regression model has been added, a time series cross validation is also used in it.\n",
        "\n",
        "IF YOU WANT TO DO ONLY EVALUATION AND NOT TUNING, DON'T RUN THE CELLS WITH RANDOMIZED AND GRID SEARCH. USE THE COMMENT VERSION OF THE PARAMETERS THAT IS SAVED IN THE CELLS BELOW.\n",
        "\n",
        "If I remember correctly what the teachers said in the first lesson, the persformance of our model will not be much better than flipping a coin, so keep that in mind when evaluating the models, they are unlikely to be outstandingly good. Honestly, if they are too good, maybe let the rest of the team now, because there may be some data leakage happening.\n",
        "\n",
        "To enhance the performance, maybe playing around with the decision threshold could help, but I will leave that up to Zahra and her tuning efforts. Please, also check the feature importance for the models and it something appears suspiciously important, look into it (may be data leakage)\n",
        "\n",
        "#### A1 models\n",
        "There was an issue with the random forest right off the bat, because some infinity values ended up in the data. This must be fixed in preprocessing so please, go back to it and check it. It is written more in detail in the Issues section above.\n",
        "The models for A0 are created analogously to the models for A0, so there is much less text there. If I use anything different, I always write it in the comments of markdown cells by the changes!\n",
        "The searches are taking much longer on the big data so, once again, the results are written out so no one has to run it again unless the data has changed.\n",
        "\n",
        "#### Disclaimers!!!\n",
        "1)If you run the prepared parameters for the models, please put the cells back to being a comment after doing so. Otherwise, it could mess with the code. Also, I didn't actually try using it, since I ran all the grid searches, so if there is some issue, please don't be mad at me and fix it.\n",
        "2)Remember that the code and results will change when the additional preprocessing is done, but it shouldn't be too much of an issue. Just keep in mind that it may be necessary to go back to all this."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Libraries import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pKBfgP1t21gI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier, plot_tree \n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import mean_squared_error "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data import, train test split and shenanigans\n",
        "The train test split should later be replaced by just loading the already split data after it has been done in preprocessing.\n",
        "After this step, the models for A0 dataset will be made in the first chunk and A1 on the second chunk of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "FILE_PATH_a0 = Path(\"ready_data\") / \"data_a0_encoded.csv\"\n",
        "FILE_PATH_a1 = Path(\"ready_data\") / \"data_a1_encoded.csv\"\n",
        "\n",
        "data_a0 = pd.read_csv(FILE_PATH_a0)\n",
        "data_a1 = pd.read_csv(FILE_PATH_a1)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42593 entries, 0 to 42592\n",
            "Data columns (total 26 columns):\n",
            " #   Column                         Non-Null Count  Dtype  \n",
            "---  ------                         --------------  -----  \n",
            " 0   Time                           42593 non-null  int64  \n",
            " 1   Target                         42593 non-null  int64  \n",
            " 2   HomeTeam_enc                   42593 non-null  int64  \n",
            " 3   avg_goals_in_last5_home        42115 non-null  float64\n",
            " 4   avg_goals_conceded_last5_home  42115 non-null  float64\n",
            " 5   AwayTeam_enc                   42593 non-null  int64  \n",
            " 6   avg_goals_in_last5_away        42115 non-null  float64\n",
            " 7   avg_goals_conceded_last5_away  42115 non-null  float64\n",
            " 8   Year                           42593 non-null  int64  \n",
            " 9   Month                          42593 non-null  int64  \n",
            " 10  Dayofweek                      42593 non-null  int64  \n",
            " 11  Is_weekend                     42593 non-null  int64  \n",
            " 12  Season_of_year                 42593 non-null  int64  \n",
            " 13  Country_D                      42593 non-null  bool   \n",
            " 14  Country_E                      42593 non-null  bool   \n",
            " 15  Country_F                      42593 non-null  bool   \n",
            " 16  Country_G                      42593 non-null  bool   \n",
            " 17  Country_I                      42593 non-null  bool   \n",
            " 18  Country_N                      42593 non-null  bool   \n",
            " 19  Country_P                      42593 non-null  bool   \n",
            " 20  Country_SC                     42593 non-null  bool   \n",
            " 21  Country_SP                     42593 non-null  bool   \n",
            " 22  Country_T                      42593 non-null  bool   \n",
            " 23  Division_1                     42593 non-null  bool   \n",
            " 24  Division_2                     42593 non-null  bool   \n",
            " 25  Division_3                     42593 non-null  bool   \n",
            "dtypes: bool(13), float64(4), int64(9)\n",
            "memory usage: 4.8 MB\n",
            "None\n",
            "   Time  Target  HomeTeam_enc  avg_goals_in_last5_home  \\\n",
            "0    19       1           420                      NaN   \n",
            "1    19       0           101                      NaN   \n",
            "2    19       1             8                      NaN   \n",
            "3    19       1           201                      NaN   \n",
            "4    19       0           372                      NaN   \n",
            "\n",
            "   avg_goals_conceded_last5_home  AwayTeam_enc  avg_goals_in_last5_away  \\\n",
            "0                            NaN           204                      NaN   \n",
            "1                            NaN           444                      NaN   \n",
            "2                            NaN           245                      NaN   \n",
            "3                            NaN           196                      NaN   \n",
            "4                            NaN            44                      NaN   \n",
            "\n",
            "   avg_goals_conceded_last5_away  Year  Month  ...  Country_G  Country_I  \\\n",
            "0                            NaN  2019      7  ...      False      False   \n",
            "1                            NaN  2019      7  ...      False      False   \n",
            "2                            NaN  2019      7  ...      False      False   \n",
            "3                            NaN  2019      7  ...      False      False   \n",
            "4                            NaN  2019      7  ...      False      False   \n",
            "\n",
            "   Country_N  Country_P  Country_SC  Country_SP  Country_T  Division_1  \\\n",
            "0      False      False       False       False      False       False   \n",
            "1      False      False       False       False      False       False   \n",
            "2      False      False       False       False      False       False   \n",
            "3      False      False       False       False      False       False   \n",
            "4      False      False       False       False      False       False   \n",
            "\n",
            "   Division_2  Division_3  \n",
            "0        True       False  \n",
            "1        True       False  \n",
            "2        True       False  \n",
            "3        True       False  \n",
            "4        True       False  \n",
            "\n",
            "[5 rows x 26 columns]\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42593 entries, 0 to 42592\n",
            "Data columns (total 32 columns):\n",
            " #   Column                         Non-Null Count  Dtype  \n",
            "---  ------                         --------------  -----  \n",
            " 0   Time                           42593 non-null  int64  \n",
            " 1   Target                         42593 non-null  int64  \n",
            " 2   HomeTeam_enc                   42593 non-null  int64  \n",
            " 3   avg_goals_in_last5_home        42115 non-null  float64\n",
            " 4   avg_goals_conceded_last5_home  42115 non-null  float64\n",
            " 5   AwayTeam_enc                   42593 non-null  int64  \n",
            " 6   avg_goals_in_last5_away        42115 non-null  float64\n",
            " 7   avg_goals_conceded_last5_away  42115 non-null  float64\n",
            " 8   market_decisiveness            42546 non-null  float64\n",
            " 9   expected_total_goals           42546 non-null  float64\n",
            " 10  Norm_Ah_P_home                 42546 non-null  float64\n",
            " 11  Norm_Ah_P_away                 42546 non-null  float64\n",
            " 12  ah_imbalance                   42546 non-null  float64\n",
            " 13  ah_market_confidence           42546 non-null  float64\n",
            " 14  Year                           42593 non-null  int64  \n",
            " 15  Month                          42593 non-null  int64  \n",
            " 16  Dayofweek                      42593 non-null  int64  \n",
            " 17  Is_weekend                     42593 non-null  int64  \n",
            " 18  Season_of_year                 42593 non-null  int64  \n",
            " 19  Country_D                      42593 non-null  bool   \n",
            " 20  Country_E                      42593 non-null  bool   \n",
            " 21  Country_F                      42593 non-null  bool   \n",
            " 22  Country_G                      42593 non-null  bool   \n",
            " 23  Country_I                      42593 non-null  bool   \n",
            " 24  Country_N                      42593 non-null  bool   \n",
            " 25  Country_P                      42593 non-null  bool   \n",
            " 26  Country_SC                     42593 non-null  bool   \n",
            " 27  Country_SP                     42593 non-null  bool   \n",
            " 28  Country_T                      42593 non-null  bool   \n",
            " 29  Division_1                     42593 non-null  bool   \n",
            " 30  Division_2                     42593 non-null  bool   \n",
            " 31  Division_3                     42593 non-null  bool   \n",
            "dtypes: bool(13), float64(10), int64(9)\n",
            "memory usage: 6.7 MB\n",
            "None\n",
            "   Time  Target  HomeTeam_enc  avg_goals_in_last5_home  \\\n",
            "0    19       0           307                      NaN   \n",
            "1    19       1           184                      NaN   \n",
            "2    19       0           372                      NaN   \n",
            "3    19       1           420                      NaN   \n",
            "4    19       1           201                      NaN   \n",
            "\n",
            "   avg_goals_conceded_last5_home  AwayTeam_enc  avg_goals_in_last5_away  \\\n",
            "0                            NaN           435                      1.4   \n",
            "1                            NaN           238                      1.0   \n",
            "2                            NaN            44                      1.4   \n",
            "3                            NaN           204                      1.2   \n",
            "4                            NaN           196                      0.8   \n",
            "\n",
            "   avg_goals_conceded_last5_away  market_decisiveness  expected_total_goals  \\\n",
            "0                            2.0             0.093350              1.677022   \n",
            "1                            1.2             0.121554              1.958666   \n",
            "2                            2.2             0.114610              1.644545   \n",
            "3                            0.6             0.066234              1.890909   \n",
            "4                            0.8             0.070312              1.710794   \n",
            "\n",
            "   ...  Country_G  Country_I  Country_N  Country_P  Country_SC  Country_SP  \\\n",
            "0  ...      False      False      False      False       False       False   \n",
            "1  ...      False      False      False      False       False       False   \n",
            "2  ...      False      False      False      False       False       False   \n",
            "3  ...      False      False      False      False       False       False   \n",
            "4  ...      False      False      False      False       False       False   \n",
            "\n",
            "   Country_T  Division_1  Division_2  Division_3  \n",
            "0      False       False        True       False  \n",
            "1      False        True       False       False  \n",
            "2      False       False        True       False  \n",
            "3      False       False        True       False  \n",
            "4      False       False        True       False  \n",
            "\n",
            "[5 rows x 32 columns]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'\\nThe averaged data of the last 5 games contains missing values. \\nThis is because for the first 5 matches, it is always impossible to compute the average.\\nBecause this is a derived column, this makes sense and should not be an issue for the data.\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#check that everything checks out\n",
        "print(data_a0.info())\n",
        "print(data_a0.head())\n",
        "\n",
        "print(data_a1.info())\n",
        "print(data_a1.head())\n",
        "\n",
        "'''\n",
        "The averaged data of the last 5 games contains missing values. \n",
        "This is because for the first 5 matches, it is always impossible to compute the average.\n",
        "Because this is a derived column, this makes sense and should not be an issue for the data.\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The country and division dummies are booleans, change that into numerical."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_a0[data_a0.select_dtypes(include='bool').columns]=data_a0[data_a0.select_dtypes(include='bool').columns].astype(int)\n",
        "data_a1[data_a1.select_dtypes(include='bool').columns]=data_a1[data_a1.select_dtypes(include='bool').columns].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "#check that all columns have only numerical values\n",
        "non_numeric_cols0 = data_a0.select_dtypes(exclude=[np.number]).columns\n",
        "non_numeric_cols1 = data_a1.select_dtypes(exclude=[np.number]).columns\n",
        "\n",
        "assert len(non_numeric_cols0)==0\n",
        "assert len(non_numeric_cols1)==0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data has been sorted chronologically in the preprocessing phase. Because this data is a time series and is likely time dependend, we will not be doing a random split of the data, but rather, a chronological one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "#just a check to see that we are good to keep working with the data and it's in the form we want\n",
        "assert type(data_a0)==pd.core.frame.DataFrame\n",
        "assert type(data_a1)==pd.core.frame.DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "#train test split, 80/20 ratio\n",
        "#for A0\n",
        "split_index_0 = int(0.8 * len(data_a0))\n",
        "\n",
        "train0 = data_a0.iloc[:split_index_0]\n",
        "test0  = data_a0.iloc[split_index_0:]\n",
        "\n",
        "X_train_0, y_train_0 = train0.drop(columns='Target'), train0['Target']\n",
        "X_test_0,  y_test_0  = test0.drop(columns='Target'),  test0['Target']\n",
        "\n",
        "#for A1\n",
        "split_index_1 = int(0.8 * len(data_a1))\n",
        "\n",
        "train1 = data_a1.iloc[:split_index_1]\n",
        "test1  = data_a1.iloc[split_index_1:]\n",
        "\n",
        "X_train_1, y_train_1 = train1.drop(columns='Target'), train1['Target']\n",
        "X_test_1,  y_test_1  = test1.drop(columns='Target'),  test1['Target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model creation A0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RandomForestClassifier\n",
        "Because this is a classification task and we aren't looking at a continuous target variable, we will use the RandomForestClassifier and not the RandomForestRegressor we have used in the lectures.\n",
        "\n",
        "Because we have derived the avg variables, there is some missingness in the data. I will drop the observations with missing values for now but I think it can be fixed (check author's notes at the top of this markdown)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "#temporary solution to missing values\n",
        "X_train_0 = X_train_0.dropna()\n",
        "y_train_0 = y_train_0.loc[X_train_0.index]\n",
        "\n",
        "X_test_0 = X_test_0.dropna()\n",
        "y_test_0 = y_test_0.loc[X_test_0.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "#initiate the model\n",
        "rf0_1 = RandomForestClassifier(random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "#because the data is chronological, we cannot do a randomized cross-validation when choosing the model\n",
        "#we will use a rolling cross-validation instead\n",
        "\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "tscv = TimeSeriesSplit(n_splits=5) \n",
        "\n",
        "#this will split out data into 5 folds and we will always evaluate only based on the past, preventing leakage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#random search for some good values to use in gridsearch (hyperparameter tuning)\n",
        "#parameters to go through\n",
        "param_grid= {\n",
        "    'max_depth':[i for i in range(1, 30)],\n",
        "    'min_samples_split':[i for i in range(1,300)],\n",
        "    'min_samples_leaf':[i for i in range(1, 200)],\n",
        "    'criterion' :['gini', 'entropy', 'log_loss']\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'max_depth': [18, 16, 14, 18], 'min_samples_split': [268, 239, 291, 282], 'min_samples_leaf': [58, 43, 61, 66], 'criterion': ['entropy', 'entropy', 'gini', 'log_loss']}\n"
          ]
        }
      ],
      "source": [
        "#the random search\n",
        "params={\n",
        "    'max_depth':[],\n",
        "    'min_samples_split':[],\n",
        "    'min_samples_leaf':[],\n",
        "    'criterion' :[]\n",
        "} #empty parameter grid to input the results of the random search\n",
        "for state in [1, 20, 42, 200]: \n",
        "    #the loop numbers were randomly chosen, they are just here as random seeds to make the results reproducible\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=rf0_1,\n",
        "        param_distributions=param_grid,\n",
        "        cv=tscv,\n",
        "        n_iter=100,\n",
        "        random_state=state,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train_0, y_train_0)\n",
        "    new_params=random_search.best_params_\n",
        "    for key, value in new_params.items():\n",
        "        params[key].append(value)\n",
        "\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'max_depth': [16, 18, 14], 'min_samples_split': [282, 291, 268, 239], 'min_samples_leaf': [66, 58, 43, 61], 'criterion': ['log_loss', 'gini', 'entropy']}\n"
          ]
        }
      ],
      "source": [
        "#keeping only unique values in the parameter grid\n",
        "for key in params:\n",
        "    params[key] = list(set(params[key]))\n",
        "print(params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The random search is run multiple times to find some values that could be used in the grid search. As this takes some time, here are the values that it gave me when I ran the code (so they can be used immediately and without running the code):\n",
        "\n",
        "params={'max_depth': [16, 18, 14],'min_samples_split': [282, 291, 268, 239], 'min_samples_leaf': [66, 58, 43, 61], 'criterion': ['log_loss', 'gini', 'entropy']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nparams={'max_depth': [16, 18, 14],\\n        'min_samples_split': [282, 291, 268, 239],\\n        'min_samples_leaf': [66, 58, 43, 61],\\n        'criterion': ['log_loss', 'gini', 'entropy']}\\n\""
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#for convenience, the params output can be loaded here\n",
        "'''\n",
        "params={'max_depth': [16, 18, 14],\n",
        "        'min_samples_split': [282, 291, 268, 239],\n",
        "        'min_samples_leaf': [66, 58, 43, 61],\n",
        "        'criterion': ['log_loss', 'gini', 'entropy']}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'criterion': 'log_loss', 'max_depth': 14, 'min_samples_leaf': 61, 'min_samples_split': 282}\n"
          ]
        }
      ],
      "source": [
        "#grid search\n",
        "grid=GridSearchCV(estimator=rf0_1,\n",
        "                  param_grid=params,\n",
        "                  n_jobs=-1, \n",
        "                  cv=tscv)\n",
        "grid.fit(X_train_0, y_train_0)\n",
        "\n",
        "best_params=grid.best_params_\n",
        "print(best_params)\n",
        "\n",
        "rf_0=grid.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As grid search takes a lot of time to be computed, here are the hyperparameter values for the best estimator for future convenience.\n",
        "\n",
        "best_params={'criterion': 'log_loss', 'max_depth': 14, 'min_samples_leaf': 61, 'min_samples_split': 282}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for convenience, the best estimator can be loaded here\n",
        "'''\n",
        "best_params={'criterion': 'log_loss', 'max_depth': 14, 'min_samples_leaf': 61, 'min_samples_split': 282}\n",
        "rf_0=RandomForestClassifier(best_params, random_state=42)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Random forest model predictions\n",
        "pred_rf_0=rf_0.predict(X_test_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tree boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This process is mostly analogous to the random forest, only using a different classifier and different hyperparameters (since we need different things for the two). Otherwise, the model creation has the same steps and uses the time series cross validation that has already been used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "import scipy.stats as st"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb_parameter_grid= {\n",
        "    'n_estimators': st.randint(2, 300),\n",
        "    'max_depth': st.randint(3, 8),\n",
        "    'learning_rate': st.uniform(0.01, 0.25),\n",
        "    'subsample': st.uniform(0.8, 0.2),\n",
        "    'colsample_bytree': st.uniform(0.8, 0.2)\n",
        "} #hyperparameters for xgboost, some are from a uniform distribution because they are floats and not integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_estimators': [285, 71, 102, 154], 'max_depth': [3, 4, 5, 3], 'learning_rate': [np.float64(0.030563726508461113), np.float64(0.059119574314573674), np.float64(0.019337047187303606), np.float64(0.021098111168009435)], 'subsample': [np.float64(0.8834765592635814), np.float64(0.9215534158836575), np.float64(0.9044486520109609), np.float64(0.8412233846481453)], 'colsample_bytree': [np.float64(0.8509405508246773), np.float64(0.993252878676858), np.float64(0.8061000499878099), np.float64(0.999332288418085)]}\n"
          ]
        }
      ],
      "source": [
        "#the random search\n",
        "xgb_params={\n",
        "    'n_estimators': [],\n",
        "    'max_depth': [],\n",
        "    'learning_rate': [],\n",
        "    'subsample': [],\n",
        "    'colsample_bytree': []\n",
        "} #empty parameter grid to input the results of the random search\n",
        "for state in [1, 20, 42, 200]:\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=xgb,\n",
        "        param_distributions=xgb_parameter_grid,\n",
        "        n_iter=100,\n",
        "        cv=tscv,\n",
        "        n_jobs=-1,\n",
        "        random_state=state\n",
        "        )\n",
        "    random_search.fit(X_train_0, y_train_0)\n",
        "    new_params=random_search.best_params_\n",
        "    for key, value in new_params.items():\n",
        "        xgb_params[key].append(value)\n",
        "\n",
        "print(xgb_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "random search results:\n",
        "\n",
        "{'n_estimators': [285, 71, 102, 154], 'max_depth': [3, 4, 5, 3], 'learning_rate': [np.float64(0.030563726508461113), np.float64(0.059119574314573674), np.float64(0.019337047187303606), np.float64(0.021098111168009435)], 'subsample': [np.float64(0.8834765592635814), np.float64(0.9215534158836575), np.float64(0.9044486520109609), np.float64(0.8412233846481453)], 'colsample_bytree': [np.float64(0.8509405508246773), np.float64(0.993252878676858), np.float64(0.8061000499878099), np.float64(0.999332288418085)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for convenience, the params output can be loaded here\n",
        "'''\n",
        "xgb_params={'n_estimators': [285, 71, 102, 154], \n",
        "            'max_depth': [3, 4, 5, 3], \n",
        "            'learning_rate': [np.float64(0.030563726508461113), np.float64(0.059119574314573674), \n",
        "                              np.float64(0.019337047187303606), np.float64(0.021098111168009435)], \n",
        "            'subsample': [np.float64(0.8834765592635814), np.float64(0.9215534158836575), \n",
        "                          np.float64(0.9044486520109609), np.float64(0.8412233846481453)],\n",
        "            'colsample_bytree': [np.float64(0.8509405508246773), np.float64(0.993252878676858),\n",
        "                                  np.float64(0.8061000499878099), np.float64(0.999332288418085)]}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'colsample_bytree': np.float64(0.8061000499878099), 'learning_rate': np.float64(0.021098111168009435), 'max_depth': 5, 'n_estimators': 154, 'subsample': np.float64(0.9044486520109609)}\n"
          ]
        }
      ],
      "source": [
        "xgb_grid = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=xgb_params,\n",
        "    cv=tscv,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "xgb_grid.fit(X_train_0, y_train_0)\n",
        "\n",
        "best_params=xgb_grid.best_params_\n",
        "print(best_params)\n",
        "\n",
        "xgb_0=xgb_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best estimator parameters from GridSearchCV\n",
        "{'colsample_bytree': np.float64(0.8061000499878099), 'learning_rate': np.float64(0.021098111168009435), 'max_depth': 5, 'n_estimators': 154, 'subsample': np.float64(0.9044486520109609)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for convenience, the best estimator can be loaded here\n",
        "'''\n",
        "best_params={'colsample_bytree': np.float64(0.8061000499878099),\n",
        "             'learning_rate': np.float64(0.021098111168009435),\n",
        "             'max_depth': 5, 'n_estimators': 154, 'subsample': np.float64(0.9044486520109609)}\n",
        "xgb_0=RandomForestClassifier(best_params, random_state=42)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Random forest model predictions\n",
        "pred_xgb_0=xgb_0.predict(X_test_0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression\n",
        "Because we are predicting a binary outcome, we can use a logistic regression model, as it uses the probability of the outcome"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "\n",
        "logreg_0=LogisticRegressionCV(max_iter=10000, cv=tscv) \n",
        "#logistic regression with time series Cross Validation\n",
        "\n",
        "logreg_0.fit(X_train_0, y_train_0)\n",
        "pred_logreg=logreg_0.predict(X_test_0) #predictions for the logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model creation A1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Random Forest Classifier\n",
        "Analogous to model creation 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "#temporary solution to missing values\n",
        "X_train_1 = X_train_1.dropna()\n",
        "y_train_1 = y_train_1.loc[X_train_1.index]\n",
        "\n",
        "X_test_1 = X_test_1.dropna()\n",
        "y_test_1 = y_test_1.loc[X_test_1.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "AssertionError",
          "evalue": "there are NaNs or infinite numbers in X_train_1",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[83], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#There were some issues with the randomized search that claims there are missing or infinite values in the data\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#assert that this is not happening, if yes, fix it, if not, look for a different cause of the problem\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(X_train_1\u001b[38;5;241m.\u001b[39mto_numpy())\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthere are NaNs or infinite numbers in X_train_1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(y_train_1\u001b[38;5;241m.\u001b[39mto_numpy())\u001b[38;5;241m.\u001b[39mall(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthere are NaNs or infinite numbers in y_train_1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;31mAssertionError\u001b[0m: there are NaNs or infinite numbers in X_train_1"
          ]
        }
      ],
      "source": [
        "#There were some issues with the randomized search that claims there are missing or infinite values in the data\n",
        "#assert that this is not happening, if yes, fix it, if not, look for a different cause of the problem\n",
        "'''\n",
        "assert np.isfinite(X_train_1.to_numpy()).all(), \"there are NaNs or infinite numbers in X_train_1\"\n",
        "assert np.isfinite(y_train_1.to_numpy()).all(), \"there are NaNs or infinite numbers in y_train_1\"\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "print(X_train_1.isna().sum().sum()) #no missing values so it must be infinity\n",
        "print(X_test_1.isna().sum().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Time                             0\n",
              "HomeTeam_enc                     0\n",
              "avg_goals_in_last5_home          0\n",
              "avg_goals_conceded_last5_home    0\n",
              "AwayTeam_enc                     0\n",
              "avg_goals_in_last5_away          0\n",
              "avg_goals_conceded_last5_away    0\n",
              "market_decisiveness              0\n",
              "expected_total_goals             0\n",
              "Norm_Ah_P_home                   0\n",
              "Norm_Ah_P_away                   0\n",
              "ah_imbalance                     1\n",
              "ah_market_confidence             1\n",
              "Year                             0\n",
              "Month                            0\n",
              "Dayofweek                        0\n",
              "Is_weekend                       0\n",
              "Season_of_year                   0\n",
              "Country_D                        0\n",
              "Country_E                        0\n",
              "Country_F                        0\n",
              "Country_G                        0\n",
              "Country_I                        0\n",
              "Country_N                        0\n",
              "Country_P                        0\n",
              "Country_SC                       0\n",
              "Country_SP                       0\n",
              "Country_T                        0\n",
              "Division_1                       0\n",
              "Division_2                       0\n",
              "Division_3                       0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(X_train_1 == np.inf).sum() + (X_train_1 == -np.inf).sum() #the issue is infinity values\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Time                             0\n",
              "HomeTeam_enc                     0\n",
              "avg_goals_in_last5_home          0\n",
              "avg_goals_conceded_last5_home    0\n",
              "AwayTeam_enc                     0\n",
              "avg_goals_in_last5_away          0\n",
              "avg_goals_conceded_last5_away    0\n",
              "market_decisiveness              0\n",
              "expected_total_goals             0\n",
              "Norm_Ah_P_home                   0\n",
              "Norm_Ah_P_away                   0\n",
              "ah_imbalance                     0\n",
              "ah_market_confidence             0\n",
              "Year                             0\n",
              "Month                            0\n",
              "Dayofweek                        0\n",
              "Is_weekend                       0\n",
              "Season_of_year                   0\n",
              "Country_D                        0\n",
              "Country_E                        0\n",
              "Country_F                        0\n",
              "Country_G                        0\n",
              "Country_I                        0\n",
              "Country_N                        0\n",
              "Country_P                        0\n",
              "Country_SC                       0\n",
              "Country_SP                       0\n",
              "Country_T                        0\n",
              "Division_1                       0\n",
              "Division_2                       0\n",
              "Division_3                       0\n",
              "dtype: int64"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "(X_test_1 == np.inf).sum() + (X_test_1 == -np.inf).sum() \n",
        "#no infinities in the test set, but I'll write a code to remove it from everywhere, just to be sure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "#removing problematic infinity values (temporary solution)\n",
        "#replace with nans\n",
        "X_train_1 = X_train_1.replace([np.inf, -np.inf], np.nan)\n",
        "X_test_1 = X_test_1.replace([np.inf, -np.inf], np.nan)\n",
        "#drop\n",
        "X_train_1 = X_train_1.dropna()\n",
        "y_train_1 = y_train_1.loc[X_train_1.index]\n",
        "\n",
        "X_test_1 = X_test_1.dropna()\n",
        "y_test_1 = y_test_1.loc[X_test_1.index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "#run the asserts to make sure the issue is no longer here\n",
        "assert np.isfinite(X_train_1.to_numpy()).all(), \"there are NaNs or infinite numbers in X_train_1\"\n",
        "assert np.isfinite(y_train_1.to_numpy()).all(), \"there are NaNs or infinite numbers in y_train_1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {},
      "outputs": [],
      "source": [
        "#initiate the model\n",
        "rf1_1 = RandomForestClassifier(random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'max_depth': [15, 22, 10, 7], 'min_samples_split': [297, 190, 174, 144], 'min_samples_leaf': [128, 69, 98, 167], 'criterion': ['log_loss', 'entropy', 'gini', 'gini']}\n"
          ]
        }
      ],
      "source": [
        "#the random search\n",
        "params_1={\n",
        "    'max_depth':[],\n",
        "    'min_samples_split':[],\n",
        "    'min_samples_leaf':[],\n",
        "    'criterion' :[]\n",
        "\n",
        "} #empty parameter grid to input the results of the random search\n",
        "for state in [1, 20, 42, 200]:\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=rf1_1,\n",
        "        param_distributions=param_grid, #we use the same parameter grid for this as in model of A0\n",
        "        cv=tscv,\n",
        "        n_iter=100,\n",
        "        random_state=state,\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    random_search.fit(X_train_1, y_train_1)\n",
        "    new_params=random_search.best_params_\n",
        "    for key, value in new_params.items():\n",
        "        params_1[key].append(value)\n",
        "\n",
        "print(params_1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'max_depth': [10, 7, 22, 15], 'min_samples_split': [144, 297, 174, 190], 'min_samples_leaf': [128, 98, 69, 167], 'criterion': ['log_loss', 'gini', 'entropy']}\n"
          ]
        }
      ],
      "source": [
        "#keeping only unique values in the parameter grid\n",
        "for key in params_1:\n",
        "    params_1[key] = list(set(params_1[key]))\n",
        "print(params_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "params_1={'max_depth': [10, 7, 22, 15], 'min_samples_split': [144, 297, 174, 190], 'min_samples_leaf': [128, 98, 69, 167], 'criterion': ['log_loss', 'gini', 'entropy']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for convenience, the params output can be loaded here\n",
        "'''\n",
        "params_1={\n",
        "    'max_depth': [10, 7, 22, 15],\n",
        "    'min_samples_split': [144, 297, 174, 190],\n",
        "    'min_samples_leaf': [128, 98, 69, 167],\n",
        "    'criterion': ['log_loss', 'gini', 'entropy']}\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 128, 'min_samples_split': 297}\n"
          ]
        }
      ],
      "source": [
        "#grid search\n",
        "grid=GridSearchCV(estimator=rf1_1,\n",
        "                  param_grid=params_1,\n",
        "                  n_jobs=-1, \n",
        "                  cv=tscv)\n",
        "grid.fit(X_train_1, y_train_1)\n",
        "\n",
        "best_params=grid.best_params_\n",
        "print(best_params)\n",
        "\n",
        "rf_1=grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for convenience, the best estimator can be loaded here\n",
        "'''\n",
        "best_params={'criterion': 'log_loss', 'max_depth': 15, 'min_samples_leaf': 128, 'min_samples_split': 297}\n",
        "rf_1=RandomForestClassifier(best_params, random_state=42)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_rf_1=rf_1.predict(X_test_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tree boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analogous to model A0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {},
      "outputs": [],
      "source": [
        "xgb1_1 = XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric='logloss',\n",
        "    use_label_encoder=False,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'n_estimators': [32, 50, 202, 154], 'max_depth': [3, 4, 3, 3], 'learning_rate': [np.float64(0.06955272909778563), np.float64(0.02068706309628359), np.float64(0.03257244251360208), np.float64(0.021098111168009435)], 'subsample': [np.float64(0.8562433791337292), np.float64(0.8405358828849236), np.float64(0.8641560129943472), np.float64(0.8412233846481453)], 'colsample_bytree': [np.float64(0.9411364273423346), np.float64(0.9048980247717036), np.float64(0.9071549368149517), np.float64(0.999332288418085)]}\n"
          ]
        }
      ],
      "source": [
        "#the random search\n",
        "xgb_params={\n",
        "    'n_estimators': [],\n",
        "    'max_depth': [],\n",
        "    'learning_rate': [],\n",
        "    'subsample': [],\n",
        "    'colsample_bytree': []\n",
        "} #empty parameter grid to input the results of the random search\n",
        "for state in [1, 20, 42, 200]:\n",
        "    random_search = RandomizedSearchCV(\n",
        "        estimator=xgb1_1,\n",
        "        param_distributions=xgb_parameter_grid, #use the same grid as before\n",
        "        n_iter=100,\n",
        "        cv=tscv,\n",
        "        n_jobs=-1,\n",
        "        random_state=state\n",
        "        )\n",
        "    random_search.fit(X_train_1, y_train_1)\n",
        "    new_params=random_search.best_params_\n",
        "    for key, value in new_params.items():\n",
        "        xgb_params[key].append(value)\n",
        "\n",
        "print(xgb_params)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Best parameters:\n",
        "\n",
        "{'n_estimators': [32, 50, 202, 154], 'max_depth': [3, 4, 3, 3], 'learning_rate': [np.float64(0.06955272909778563), np.float64(0.02068706309628359), np.float64(0.03257244251360208), np.float64(0.021098111168009435)], 'subsample': [np.float64(0.8562433791337292), np.float64(0.8405358828849236), np.float64(0.8641560129943472), np.float64(0.8412233846481453)], 'colsample_bytree': [np.float64(0.9411364273423346), np.float64(0.9048980247717036), np.float64(0.9071549368149517), np.float64(0.999332288418085)]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load parameters for convenience\n",
        "'''\n",
        "xgb_params={'n_estimators': [32, 50, 202, 154],\n",
        "             'max_depth': [3, 4, 3, 3],\n",
        "             'learning_rate': [np.float64(0.06955272909778563), np.float64(0.02068706309628359),\n",
        "                               np.float64(0.03257244251360208), np.float64(0.021098111168009435)],\n",
        "             'subsample': [np.float64(0.8562433791337292), np.float64(0.8405358828849236),\n",
        "                           np.float64(0.8641560129943472), np.float64(0.8412233846481453)],\n",
        "             'colsample_bytree': [np.float64(0.9411364273423346), np.float64(0.9048980247717036),\n",
        "                                  np.float64(0.9071549368149517), np.float64(0.999332288418085)]}\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'colsample_bytree': np.float64(0.9048980247717036), 'learning_rate': np.float64(0.03257244251360208), 'max_depth': 4, 'n_estimators': 32, 'subsample': np.float64(0.8405358828849236)}\n"
          ]
        }
      ],
      "source": [
        "xgb_grid = GridSearchCV(\n",
        "    estimator=xgb1_1,\n",
        "    param_grid=xgb_params,\n",
        "    cv=tscv,\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "xgb_grid.fit(X_train_1, y_train_1)\n",
        "\n",
        "best_params=xgb_grid.best_params_\n",
        "print(best_params)\n",
        "\n",
        "xgb_1=xgb_grid.best_estimator_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#for convenience, best parameters and model\n",
        "'''\n",
        "best_params={'colsample_bytree': np.float64(0.9048980247717036),\n",
        "             'learning_rate': np.float64(0.03257244251360208),\n",
        "             'max_depth': 4, 'n_estimators': 32, 'subsample': np.float64(0.8405358828849236)}\n",
        "xgb_1=RandomForestClassifier(best_params, random_state=42)\n",
        "'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Random forest model predictions\n",
        "pred_xgb_1=xgb_1.predict(X_test_1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Analogous to model A0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {},
      "outputs": [],
      "source": [
        "logreg_1=LogisticRegressionCV(max_iter=10000, cv=tscv) \n",
        "#logistic regression with time series Cross Validation\n",
        "\n",
        "logreg_1.fit(X_train_1, y_train_1)\n",
        "pred_logreg_1=logreg_1.predict(X_test_1) #predictions for the logistic regression"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
